{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install \n",
    "\n",
    "# Download this repository\n",
    "#!git clone https://github.com/pluskal-lab/DreaMS.git\n",
    "#!cd DreaMS\n",
    "\n",
    "# Create conda environment\n",
    "#!conda update -n base -c defaults conda\n",
    "#!conda create -n dreams python==3.11.0 --yes\n",
    "#!conda init\n",
    "#!conda activate dreams\n",
    "\n",
    "# Install DreaMS\n",
    "#%pip install -e ./DreaMS\n",
    "\n",
    "#%pip install pytorch-lightning\n",
    "#!git clone https://github.com/colorfulcereal/MassSpecGym\n",
    "#%pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install -e MassSpecGym\n",
    "\n",
    "## How to enable GPU support for TensorFlow or PyTorch on MacOS\n",
    "## https://medium.com/bluetuple-ai/how-to-enable-gpu-support-for-tensorflow-or-pytorch-on-macos-4aaaad057e74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MassSpecGym dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from massspecgym.utils import load_massspecgym\n",
    "df = load_massspecgym()\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MS/MS EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot spectra\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Function to plot histogram of mzs and intensities\n",
    "def plot_spectrum(mzs, intensities):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.stem(mzs, intensities, basefmt='-')\n",
    "    plt.title('Mass Spectrum')\n",
    "    plt.xlabel('m/z')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.show()\n",
    "\n",
    "# Plot a random spectrum\n",
    "random_index = np.random.randint(0, len(df))\n",
    "print(random_index)\n",
    "mzs = df.iloc[random_index]['mzs']\n",
    "intensities = df.iloc[random_index]['intensities']\n",
    "plot_spectrum(mzs, intensities)\n",
    "\n",
    "# Plot multiple spectra\n",
    "num_spectra = 5\n",
    "random_indices = np.random.randint(0, len(df), num_spectra)\n",
    "fig, axs = plt.subplots(nrows=num_spectra, ncols=1, figsize=(8, 6*num_spectra))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    mzs = df.iloc[idx]['mzs']\n",
    "    intensities = df.iloc[idx]['intensities']\n",
    "    axs[i].stem(mzs, intensities, basefmt='-')\n",
    "    axs[i].set_title(f'Spectrum {idx}')\n",
    "    axs[i].set_xlabel('m/z')\n",
    "    axs[i].set_ylabel('Intensity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Halogen symbols\n",
    "halogens = ['F', 'Cl', 'Br', 'I', 'At', 'Ts']\n",
    "\n",
    "# Initialize halogen count dictionary\n",
    "halogen_counts = {halogen: 0 for halogen in halogens}\n",
    "\n",
    "# Count mass spectra containing each halogen\n",
    "for index, row in df.iterrows():\n",
    "    for halogen in halogens:\n",
    "        if halogen in row['precursor_formula'] or halogen in row['formula']:\n",
    "            if halogen == 'F':\n",
    "                print(index)\n",
    "            halogen_counts[halogen] += 1\n",
    "\n",
    "# Calculate percentages\n",
    "halogen_percentages = {halogen: (count/len(df))*100 for halogen, count in halogen_counts.items()}\n",
    "\n",
    "# Print results\n",
    "print(\"Halogen Percentages:\")\n",
    "for halogen, percentage in halogen_percentages.items():\n",
    "    print(f\"{halogen}: {percentage:.2f}%\")\n",
    "\n",
    "for halogen, count in halogen_counts.items():\n",
    "    print(f\"{halogen}: {count}\")\n",
    "\n",
    "# Plot (%)\n",
    "# plt.bar(halogen_percentages.keys(), halogen_percentages.values())\n",
    "# plt.xlabel('Halogen')\n",
    "# plt.ylabel('Percentage (%)')\n",
    "# plt.title('Distribution of Halogens in Mass Spectra')\n",
    "# plt.ylim(0, 50)  # Set y-axis limit to 100%\n",
    "# plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: f\"{x:.0f}%\"))  # Format y-axis ticks as percentages\n",
    "# plt.show()\n",
    "\n",
    "# Plot results (count)\n",
    "plt.bar(halogen_counts.keys(), halogen_counts.values())\n",
    "plt.xlabel('Halogen')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Halogens in Mass Spectra By Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy, sys\n",
    "\n",
    "from massspecgym.data import RetrievalDataset, MassSpecDataModule\n",
    "from massspecgym.data.transforms import SpecTokenizer, MolFingerprinter\n",
    "from massspecgym.models.base import Stage\n",
    "from massspecgym.models.retrieval.base import RetrievalMassSpecGymModel\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDeepSetsRetrievalModel(RetrievalMassSpecGymModel):\n",
    "    # constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int = 128,\n",
    "        out_channels: int = 4096,  # fingerprint size\n",
    "        # out_channels: int = 4096,  # fingerprint size\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Implement your architecture.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implement your prediction logic.\"\"\"\n",
    "        x = self.phi(x)\n",
    "        x = x.sum(dim=-2)  # sum over peaks\n",
    "        x = self.rho(x)\n",
    "        return x\n",
    "\n",
    "    def step(\n",
    "        self, batch: dict, stage: Stage\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implement your custom logic of using predictions for training and inference.\"\"\"\n",
    "        # Unpack inputs\n",
    "        x = batch[\"spec\"]  # input spectra\n",
    "        fp_true = batch[\"mol\"]  # true fingerprints\n",
    "        cands = batch[\"candidates\"]  # candidate fingerprints concatenated for a batch\n",
    "        #print(cands)\n",
    "        batch_ptr = batch[\"batch_ptr\"]  # number of candidates per sample in a batch\n",
    "        #print(batch_ptr)\n",
    "\n",
    "        # Predict fingerprint\n",
    "        fp_pred = self.forward(x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = nn.functional.mse_loss(fp_true, fp_pred)\n",
    "\n",
    "        # Calculate final similarity scores between predicted fingerprints and retrieval candidates\n",
    "        fp_pred_repeated = fp_pred.repeat_interleave(batch_ptr, dim=0)\n",
    "        scores = nn.functional.cosine_similarity(fp_pred_repeated, cands)\n",
    "\n",
    "        return dict(loss=loss, scores=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init hyperparameters\n",
    "n_peaks = 10\n",
    "fp_size = 4096\n",
    "batch_size = 2\n",
    "\n",
    "# Load dataset\n",
    "dataset = RetrievalDataset(\n",
    "    spec_transform=SpecTokenizer(n_peaks=n_peaks),\n",
    "    mol_transform=MolFingerprinter(fp_size=fp_size),\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "# Init model\n",
    "model = MyDeepSetsRetrievalModel(out_channels=fp_size)\n",
    "\n",
    "# Init trainer\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\n",
    "trainer = Trainer(accelerator=\"auto\", devices=\"auto\", max_epochs=1, logger=tb_logger)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluoride Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy, sys\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "\n",
    "from massspecgym.data import MassSpecDataset, MassSpecDataModule\n",
    "from massspecgym.data.transforms import SpecTokenizer, MolFingerprinter\n",
    "from massspecgym.models.base import Stage\n",
    "from massspecgym.models.retrieval.base import MassSpecGymModel\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from massspecgym.models.base import Stage\n",
    "from dreams.api import PreTrainedModel\n",
    "from dreams.models.dreams.dreams import DreaMS as DreaMSModel\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from massspecgym.data.transforms import MolToHalogensVector\n",
    "\n",
    "# Example usage\n",
    "checker = MolToHalogensVector()\n",
    "smiles_string = \"CC(F)(F)F\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)\n",
    "# Example usage\n",
    "smiles_string = \"CCBr\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    mgf_pth = Path(\"/teamspace/studios/this_studio/MassSpecGym/data/debug/example_5_spectra.mgf\")\n",
    "    split_pth = Path(\"/teamspace/studios/this_studio/MassSpecGym/data/debug/example_5_spectra_split.tsv\")\n",
    "else:\n",
    "    mgf_pth = None\n",
    "    split_pth = None\n",
    "\n",
    "# Check if MPS is available, otherwise use CUDA\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "else:\n",
    "    mps_device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model containing the network definition\n",
    "class HalogenDetectorDreamsTest(MassSpecGymModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float=0.25,\n",
    "        gamma: float=0.5,\n",
    "        batch_size: int=64,\n",
    "        threshold: float=0.5,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if mps_device is not None:\n",
    "            self.alpha = torch.tensor([1-alpha, alpha], device=mps_device)\n",
    "        else:\n",
    "            self.alpha = torch.tensor([1-alpha, alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.threshold = threshold\n",
    "        print(f\"Training with threshold: {self.threshold}, alpha: {self.alpha}, gamma: {self.gamma}, batch_size: {self.batch_size}\")\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_precision = BinaryPrecision()\n",
    "        self.train_recall = BinaryRecall()\n",
    "        self.val_precision = BinaryPrecision()\n",
    "        self.val_recall = BinaryRecall()\n",
    "\n",
    "        # loading the DreaMS model weights from the internet\n",
    "        self.main_model = PreTrainedModel.from_ckpt(\n",
    "            # ckpt_path should be replaced with the path to the ssl_model.ckpt model downloaded from https://zenodo.org/records/10997887\n",
    "            ckpt_path=\"https://zenodo.org/records/10997887/files/ssl_model.ckpt?download=1\", ckpt_cls=DreaMSModel, n_highest_peaks=60\n",
    "        ).model.train()\n",
    "        self.lin_out = nn.Linear(1024, 1) # for F\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_main_model = self.main_model(x)[:, 0, :] # to get the precursor peak token embedding \n",
    "        fl_probability = F.sigmoid(self.lin_out(output_main_model))\n",
    "        return fl_probability\n",
    "\n",
    "    def step(\n",
    "        self, batch: dict, stage: Stage\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implement your custom logic of using predictions for training and inference.\"\"\"\n",
    "        # Unpack inputs\n",
    "        x = batch[\"spec\"]  # shape: [batch_size, num_peaks + 1, 2]\n",
    "        #print(\"--batch.keys\", batch.keys())\n",
    "\n",
    "        halogen_vector_true = batch[\"mol\"] # shape: [batch_size, 4]\n",
    "        # Extract the 1st column --> fluorine predictions\n",
    "        true_values = halogen_vector_true[:, 0] # shape [batch_size]\n",
    "\n",
    "        # the forward pass\n",
    "        predicted_probs = self.forward(x) # shape [batch_size x 1]\n",
    "        \n",
    "        if DEBUG:\n",
    "            predicted_probs = predicted_probs[0] # for testing\n",
    "        else:\n",
    "            predicted_probs = predicted_probs.squeeze() # shape [batch_size]\n",
    "\n",
    "        #print(\"--predicted_probs\", predicted_probs)\n",
    "\n",
    "        ### Focal Loss: https://amaarora.github.io/posts/2020-06-29-FocalLoss.html ### \n",
    "        # Increase loss for minority misclassification (F = 1 but predicted as 0) and \n",
    "        # decreases loss for majority class misclassification (F = 0 but predicted as 1)\n",
    "        # Our MassSpecGym training data is skewed with only 5% of molecules containing Fluorine\n",
    "       \n",
    "        bce_loss = nn.BCELoss(reduction='none')\n",
    "        loss = bce_loss(predicted_probs, true_values)\n",
    "        targets = true_values.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-loss)\n",
    "        F_loss = at * (1 - pt)**self.gamma * loss\n",
    "        return { 'loss': F_loss.mean() } \n",
    "\n",
    "    def on_batch_end(\n",
    "        self, outputs: [], batch: dict, batch_idx: int, stage: Stage\n",
    "    ) -> None:\n",
    "        x = batch[\"spec\"] # shape: [batch_size, num_peaks + 1, 2]\n",
    "        halogen_vector_true = batch[\"mol\"] # shape [batch_size]\n",
    "        # updated predictions with the updated weights at the end of the batch\n",
    "        pred_probs = self.forward(x) # shape [batch_size x 1]\n",
    "\n",
    "        # thresholding\n",
    "        halogen_vector_pred_binary = torch.where(pred_probs >= self.threshold, 1, 0)\n",
    "\n",
    "        # Extract the 1st column --> fluorine predictions\n",
    "        true_labels = halogen_vector_true[:, 0] # shape [batch_size]\n",
    "        # make shape [batch_size x 1] into shape [batch_size]\n",
    "        pred_bool_labels = halogen_vector_pred_binary.squeeze() # shape [batch_size]\n",
    "\n",
    "        if stage.to_pref() == 'train_':\n",
    "            self.train_precision.update(pred_bool_labels, true_labels)\n",
    "            self.train_recall.update(pred_bool_labels, true_labels)\n",
    "        elif stage.to_pref() == 'val_':\n",
    "            self.val_precision.update(pred_bool_labels, true_labels)\n",
    "            self.val_recall.update(pred_bool_labels, true_labels)\n",
    "\n",
    "        self.log_dict({ f\"{stage.to_pref()}/loss\": outputs['loss'] },\n",
    "                prog_bar=True,\n",
    "                on_epoch=True,\n",
    "                batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def _reset_metrics_train(self):\n",
    "        # Reset states for next epoch\n",
    "        self.train_precision.reset()\n",
    "        self.train_recall.reset()\n",
    "\n",
    "    def _reset_metrics_val(self):\n",
    "        # Reset states for next epoch\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.all_predicted_probs = []  # reset the list of predicted probabilities for validation\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        self._reset_metrics_train()\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self._reset_metrics_val()\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        precision = self.train_precision.compute()\n",
    "        recall = self.train_recall.compute()\n",
    "        f1_score = (2*precision*recall)/(precision + recall) if (precision + recall) != 0 else 0\n",
    "        self.log_dict({\n",
    "                f\"train_/precision\": precision,\n",
    "                f\"train_/recall\": recall,\n",
    "                f\"train_/f1_score\": f1_score\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False\n",
    "        )\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        precision = self.val_precision.compute()\n",
    "        recall = self.val_recall.compute()\n",
    "        f1_score = (2*precision*recall)/(precision + recall) if (precision + recall) != 0 else 0\n",
    "        self.log_dict({\n",
    "                f\"val_/precision\": precision,\n",
    "                f\"val_/recall\": recall,\n",
    "                f\"val_/f1_score\": f1_score\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed adduct due to a str error\n",
    "class TestMassSpecDataset(MassSpecDataset):\n",
    "\n",
    "    def __getitem__(\n",
    "        self, i: int, transform_spec: bool = True, transform_mol: bool = True\n",
    "    ) -> dict:\n",
    "        spec = self.spectra[i]\n",
    "        metadata = self.metadata.iloc[i]\n",
    "        mol = metadata[\"smiles\"]\n",
    "\n",
    "        # Apply all transformations to the spectrum\n",
    "        item = {}\n",
    "        if transform_spec and self.spec_transform:\n",
    "            if isinstance(self.spec_transform, dict):\n",
    "                for key, transform in self.spec_transform.items():\n",
    "                    item[key] = transform(spec) if transform is not None else spec\n",
    "            else:\n",
    "                item[\"spec\"] = self.spec_transform(spec)\n",
    "        else:\n",
    "            item[\"spec\"] = spec\n",
    "\n",
    "        # Apply all transformations to the molecule\n",
    "        if transform_mol and self.mol_transform:\n",
    "            if isinstance(self.mol_transform, dict):\n",
    "                for key, transform in self.mol_transform.items():\n",
    "                    item[key] = transform(mol) if transform is not None else mol\n",
    "            else:\n",
    "                item[\"mol\"] = self.mol_transform(mol)\n",
    "        else:\n",
    "            item[\"mol\"] = mol\n",
    "\n",
    "        # Add other metadata to the item\n",
    "        item.update({\n",
    "            k: metadata[k] for k in [\"precursor_mz\"] # removed adduct due to a str error\n",
    "        })\n",
    "\n",
    "        if self.return_mol_freq:\n",
    "            item[\"mol_freq\"] = metadata[\"mol_freq\"]\n",
    "\n",
    "        if self.return_identifier:\n",
    "            item[\"identifier\"] = metadata[\"identifier\"]\n",
    "\n",
    "        # TODO: this should be refactored\n",
    "        for k, v in item.items():\n",
    "            if not isinstance(v, str):\n",
    "                item[k] = torch.as_tensor(v, dtype=self.dtype)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Init hyperparameters\n",
    "max_epochs = 1\n",
    "n_peaks = 60\n",
    "threshold = 0.3 # 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\n",
    "alpha = 0.25 # 0.25, 0.5, 0.75, 1 - found 0.25 as best\n",
    "gamma = 0.75 # 0.25, 0.5, 0.75, 1 - found 0.75 as best\n",
    "lr = 1e-5\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 64\n",
    "\n",
    "# Load dataset\n",
    "dataset = TestMassSpecDataset(\n",
    "    spec_transform=SpecTokenizer(n_peaks=n_peaks),\n",
    "    mol_transform = MolToHalogensVector(),\n",
    "    pth='/teamspace/studios/this_studio/merged_massspec_nist20_with_fold.tsv'\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    split_pth=split_pth,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = HalogenDetectorDreamsTest(\n",
    "    threshold=threshold,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr\n",
    ")\n",
    "# initialise the wandb logger and name your wandb project\n",
    "#wandb_logger = WandbLogger(project='HalogenDetection-FocalLoss-MergedMassSpecNIST20')\n",
    "wandb_logger = WandbLogger(project='HalogenDetection-FocalLoss-Test')\n",
    "# add your batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = batch_size\n",
    "wandb_logger.experiment.config[\"n_peaks\"] = n_peaks\n",
    "wandb_logger.experiment.config[\"threshold\"] = threshold\n",
    "wandb_logger.experiment.config[\"alpha\"] = alpha\n",
    "wandb_logger.experiment.config[\"gamma\"] = gamma\n",
    "\n",
    "trainer = Trainer(accelerator=\"auto\", devices=\"auto\", max_epochs=max_epochs, logger=wandb_logger, val_check_interval=0.1)\n",
    "\n",
    "# Validate before training\n",
    "data_module.prepare_data()  # Explicit call needed for validate before fit\n",
    "data_module.setup()  # Explicit call needed for validate before fit\n",
    "trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "# # Train\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Fluorine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dreams.utils.data import MSData\n",
    "from dreams.api import dreams_predictions, PreTrainedModel\n",
    "from dreams.models.heads.heads import BinClassificationHead\n",
    "from dreams.utils.io import append_to_stem\n",
    "\n",
    "\n",
    "def find_fluorine():\n",
    "\n",
    "    # in_pth = 'data/teo/<in_file>.mgf'  # or .mzML\n",
    "    # out_csv_pth = 'data/teo/<in_file>_f_preds.csv'\n",
    "\n",
    "    # in_pths = list(Path('/scratch/project/open-26-5/DreaMS/data/andrej/fluorine_dataset').glob('*.mzML'))\n",
    "    # model_ckpt_111k = '/scratch/project/open-26-5/DreaMS/dreams/HAS_F_1.0/Feb2025_8bs_5e-5lr_bce/epoch=30-step=111000.ckpt'\n",
    "    # model_ckpt_7k = '/scratch/project/open-26-5/DreaMS/dreams/HAS_F_1.0/Feb2025_8bs_5e-5lr_bce/epoch=1-step=7000.ckpt'\n",
    "\n",
    "    in_pth = Path('/teamspace/studios/this_studio/20250627_pa_flo_nmr_pos_norm.mzML')\n",
    "    # threshold = 0.9 model\n",
    "    model_ckpt = '/teamspace/studios/this_studio/HalogenDetection-FocalLoss-MergedMassSpecNIST20/opi4lx8s/checkpoints/epoch=0-step=8920.ckpt'\n",
    "\n",
    "    n_highest_peaks = 60\n",
    "\n",
    "    # Load model\n",
    "    model = HalogenDetectorDreamsTest.load_from_checkpoint(model_ckpt)\n",
    "    print(model)\n",
    "\n",
    "    print(f'Processing {in_pth}...')\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        msdata = MSData.load(in_pth, in_mem=True)\n",
    "    except ValueError as e:\n",
    "        print(f'Skipping {in_pth} because of {e}.')\n",
    "        return\n",
    "\n",
    "    # Compute fluorine probabilties\n",
    "    df = msdata.to_pandas()\n",
    "    \n",
    "    f_preds = dreams_predictions(\n",
    "        spectra=msdata,\n",
    "        model_ckpt=model,\n",
    "        n_highest_peaks=n_highest_peaks\n",
    "    )\n",
    "    df[f'F_preds'] = f_preds\n",
    "\n",
    "        # Store predictions\n",
    "    df.to_csv(append_to_stem(in_pth, 'F_preds').with_suffix('.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_fluorine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the network\n",
    "hidden_channels = 5\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, hidden_channels),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_channels, hidden_channels),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# Initialize the network\n",
    "net = net.float()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.ones(5, 10, 2)\n",
    "# Forward pass\n",
    "output = net(input_tensor)\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = np.array([1, 1., 1])\n",
    "pred_values = np.array([0.0, 1, 1])\n",
    "\n",
    "precision_score(true_values, pred_values)\n",
    "recall_score(true_values, pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from massspecgym.models.base import Stage\n",
    "from dreams.api import PreTrainedModel\n",
    "from dreams.models.dreams.dreams import DreaMS as DreaMSModel\n",
    "\n",
    "# Example forward pass (not needed to explicitly initialize the DataLoader if you are using MassSpecGym)\n",
    "from massspecgym.data.datasets import MassSpecDataset\n",
    "from massspecgym.data.transforms import SpecTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MassSpecDataset(\n",
    "    spec_transform=SpecTokenizer(n_peaks=n_peaks),\n",
    "    mol_transform = MolToHalogensVector()\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "model = HalogenDetectorDreams()\n",
    "\n",
    "dummy_batch = next(iter(dataloader))\n",
    "dummy_output = model(dummy_batch)\n",
    "print(dummy_output)  # Should print a tensor of shape (4, 4) containing halogen probabilties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "input = torch.tensor([[2.0, 3.0, 5.0], [2.0, 3.0, 5.0]])\n",
    "print(m(input))\n",
    "target = torch.tensor([[1.0, 0.0, 1.0], [1.0, 0.0, 0.0]])\n",
    "print(target)\n",
    "output = loss(m(input), target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_actual_positives    = np.array([1, 1, 1, 0, 0, 0, 0, 1, 1, 1])\n",
    "num_predicted_positives = np.array([1, 1, 0, 0, 1, 0, 0, 1, 1, 0])\n",
    "\n",
    "print(np.sum(np.logical_and(num_actual_positives, num_predicted_positives)))\n",
    "\n",
    "# a1 = np.array([1, 1, 0])\n",
    "# a2 = np.array([0, 0, 1])\n",
    "\n",
    "# print(np.sum(np.logical_and(a1, a2))) # should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for gpu\n",
    "if torch.backends.mps.is_available():\n",
    "   mps_device = torch.device(\"mps\")\n",
    "   x = torch.ones(1, device=mps_device)\n",
    "   print (x)\n",
    "else:\n",
    "   print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# GPU\n",
    "start_time = time.time()\n",
    "\n",
    "# syncrocnize time with cpu, otherwise only time for oflaoding data to gpu would be measured\n",
    "torch.mps.synchronize()\n",
    "\n",
    "a = torch.ones(4000,4000, device=\"mps\")\n",
    "for _ in range(200):\n",
    "   a +=a\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print( \"GPU Time: \", elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
