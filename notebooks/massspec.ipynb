{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluorine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy, sys\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "\n",
    "from massspecgym.data import MassSpecDataset, MassSpecDataModule\n",
    "from massspecgym.data.transforms import SpecTokenizer, MolFingerprinter\n",
    "from massspecgym.models.base import Stage\n",
    "from massspecgym.models.retrieval.base import MassSpecGymModel\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from massspecgym.models.base import Stage\n",
    "from dreams.api import PreTrainedModel\n",
    "from dreams.models.dreams.dreams import DreaMS as DreaMSModel\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall, BinaryAccuracy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_float32_matmul_precision('high')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from massspecgym.data.transforms import MolToHalogensVector, MolToPFASVector\n",
    "\n",
    "# Example usage\n",
    "checker = MolToHalogensVector() # creating an object of type MolToHalogensVector\n",
    "smiles_string = \"CC(F)(F)F\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)\n",
    "# Example usage\n",
    "smiles_string = \"CCBr\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)\n",
    "\n",
    "checker = MolToPFASVector()\n",
    "smiles_string = \"CC(F)(F)F\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)\n",
    "\n",
    "# Example usage\n",
    "smiles_string = \"CCBr\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    mgf_pth = Path(\"/teamspace/studios/this_studio/MassSpecGym/data/debug/example_5_spectra.mgf\")\n",
    "    split_pth = Path(\"/teamspace/studios/this_studio/MassSpecGym/data/debug/example_5_spectra_split.tsv\")\n",
    "else:\n",
    "    mgf_pth = None\n",
    "    split_pth = None\n",
    "\n",
    "# Check if MPS is available, otherwise use CUDA\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "else:\n",
    "    mps_device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model containing the network definition\n",
    "\n",
    "class HalogenDetectorDreamsTest(MassSpecGymModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float=0.25,\n",
    "        gamma: float=0.5,\n",
    "        batch_size: int=64,\n",
    "        threshold: float=0.5,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if mps_device is not None:\n",
    "            self.alpha = torch.tensor([1-alpha, alpha], device=mps_device)\n",
    "        else:\n",
    "            self.alpha = torch.tensor([1-alpha, alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.threshold = threshold\n",
    "        print(f\"Training with threshold: {self.threshold}, alpha: {self.alpha}, gamma: {self.gamma}, batch_size: {self.batch_size}\")\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_precision = BinaryPrecision()\n",
    "        self.train_recall = BinaryRecall()\n",
    "        self.val_precision = BinaryPrecision()\n",
    "        self.val_recall = BinaryRecall()\n",
    "        self.train_accuracy = BinaryAccuracy()\n",
    "        self.val_accuracy = BinaryAccuracy()\n",
    "\n",
    "        # loading the DreaMS model weights from the internet\n",
    "        self.main_model = PreTrainedModel.from_ckpt(\n",
    "            # ckpt_path should be replaced with the path to the ssl_model.ckpt model downloaded from https://zenodo.org/records/10997887\n",
    "            ckpt_path=\"https://zenodo.org/records/10997887/files/ssl_model.ckpt?download=1\", ckpt_cls=DreaMSModel, n_highest_peaks=60\n",
    "        ).model.train()\n",
    "        self.lin_out = nn.Linear(1024, 1) # for F\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_main_model = self.main_model(x)[:, 0, :] # to get the precursor peak token embedding \n",
    "        fl_probability = F.sigmoid(self.lin_out(output_main_model))\n",
    "        return fl_probability\n",
    "\n",
    "    def step(\n",
    "        self, batch: dict, stage: Stage\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implement your custom logic of using predictions for training and inference.\"\"\"\n",
    "        # Unpack inputs\n",
    "        x = batch[\"spec\"]  # shape: [batch_size, num_peaks + 1, 2]\n",
    "        #print(\"--batch.keys\", batch.keys())\n",
    "\n",
    "        halogen_vector_true = batch[\"mol\"] # shape: [batch_size, 4]\n",
    "\n",
    "        # Extract the 1st column --> fluorine predictions\n",
    "        true_values = halogen_vector_true[:, 0] # shape [batch_size]\n",
    "\n",
    "        # the forward pass\n",
    "        predicted_probs = self.forward(x) # shape [batch_size x 1]\n",
    "        \n",
    "        if DEBUG:\n",
    "            predicted_probs = predicted_probs[0] # for testing\n",
    "        else:\n",
    "            predicted_probs = predicted_probs.squeeze() # shape [batch_size]\n",
    "\n",
    "        #print(\"--predicted_probs\", predicted_probs)\n",
    "\n",
    "        ### Focal Loss: https://amaarora.github.io/posts/2020-06-29-FocalLoss.html ### \n",
    "        # Increase loss for minority misclassification (F = 1 but predicted as 0) and \n",
    "        # decreases loss for majority class misclassification (F = 0 but predicted as 1)\n",
    "        # Our MassSpecGym training data is skewed with only 5% of molecules containing Fluorine\n",
    "       \n",
    "        bce_loss = nn.BCELoss(reduction='none')\n",
    "        loss = bce_loss(predicted_probs, true_values)\n",
    "        targets = true_values.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-loss)\n",
    "        F_loss = at * (1 - pt)**self.gamma * loss\n",
    "        return { 'loss': F_loss.mean() } \n",
    "\n",
    "    def on_batch_end(\n",
    "        self, outputs: [], batch: dict, batch_idx: int, stage: Stage\n",
    "    ) -> None:\n",
    "        x = batch[\"spec\"] # shape: [batch_size, num_peaks + 1, 2]\n",
    "        halogen_vector_true = batch[\"mol\"] # shape [batch_size]\n",
    "        # updated predictions with the updated weights at the end of the batch\n",
    "        pred_probs = self.forward(x) # shape [batch_size x 1]\n",
    "\n",
    "        # thresholding\n",
    "        halogen_vector_pred_binary = torch.where(pred_probs >= self.threshold, 1, 0)\n",
    "\n",
    "        # Extract the 1st column --> fluorine predictions\n",
    "        true_labels = halogen_vector_true[:, 0] # shape [batch_size]\n",
    "        \n",
    "        # make shape [batch_size x 1] into shape [batch_size]\n",
    "        pred_bool_labels = halogen_vector_pred_binary.squeeze() # shape [batch_size]\n",
    "\n",
    "        if stage.to_pref() == 'train_':\n",
    "            self.train_precision.update(pred_bool_labels, true_labels)\n",
    "            self.train_recall.update(pred_bool_labels, true_labels)\n",
    "            self.train_accuracy.update(pred_bool_labels, true_labels)\n",
    "        elif stage.to_pref() == 'val_':\n",
    "            self.val_precision.update(pred_bool_labels, true_labels)\n",
    "            self.val_recall.update(pred_bool_labels, true_labels)\n",
    "            self.val_accuracy.update(pred_bool_labels, true_labels)\n",
    "\n",
    "        self.log_dict({ f\"{stage.to_pref()}/loss\": outputs['loss'] },\n",
    "                prog_bar=True,\n",
    "                on_epoch=True,\n",
    "                batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def _reset_metrics_train(self):\n",
    "        # Reset states for next epoch\n",
    "        self.train_precision.reset()\n",
    "        self.train_recall.reset()\n",
    "        self.train_accuracy.reset()\n",
    "\n",
    "    def _reset_metrics_val(self):\n",
    "        # Reset states for next epoch\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_accuracy.reset()\n",
    "        self.all_predicted_probs = []  # reset the list of predicted probabilities for validation\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        self._reset_metrics_train()\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self._reset_metrics_val()\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        precision = self.train_precision.compute()\n",
    "        recall = self.train_recall.compute()\n",
    "        accuracy = self.train_accuracy.compute()\n",
    "        f1_score = (2*precision*recall)/(precision + recall) if (precision + recall) != 0 else 0\n",
    "        self.log_dict({\n",
    "                f\"train_/precision\": precision,\n",
    "                f\"train_/recall\": recall,\n",
    "                f\"train_/accuracy\": accuracy,\n",
    "                f\"train_/f1_score\": f1_score\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False\n",
    "        )\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        precision = self.val_precision.compute()\n",
    "        recall = self.val_recall.compute()\n",
    "        accuracy = self.val_accuracy.compute()\n",
    "        f1_score = (2*precision*recall)/(precision + recall) if (precision + recall) != 0 else 0\n",
    "        self.log_dict({\n",
    "                f\"val_/precision\": precision,\n",
    "                f\"val_/recall\": recall,\n",
    "                f\"val_/accuracy\": accuracy,\n",
    "                f\"val_/f1_score\": f1_score\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed adduct due to a str error\n",
    "class TestMassSpecDataset(MassSpecDataset):\n",
    "\n",
    "    def __getitem__(\n",
    "        self, i: int, transform_spec: bool = True, transform_mol: bool = True\n",
    "    ) -> dict:\n",
    "        spec = self.spectra[i]\n",
    "        metadata = self.metadata.iloc[i]\n",
    "        mol = metadata[\"smiles\"]\n",
    "\n",
    "        # Apply all transformations to the spectrum\n",
    "        item = {}\n",
    "        if transform_spec and self.spec_transform:\n",
    "            if isinstance(self.spec_transform, dict):\n",
    "                for key, transform in self.spec_transform.items():\n",
    "                    item[key] = transform(spec) if transform is not None else spec\n",
    "            else:\n",
    "                item[\"spec\"] = self.spec_transform(spec)\n",
    "        else:\n",
    "            item[\"spec\"] = spec\n",
    "\n",
    "        # Apply all transformations to the molecule\n",
    "        if transform_mol and self.mol_transform:\n",
    "            if isinstance(self.mol_transform, dict):\n",
    "                for key, transform in self.mol_transform.items():\n",
    "                    item[key] = transform(mol) if transform is not None else mol\n",
    "            else:\n",
    "                item[\"mol\"] = self.mol_transform(mol)\n",
    "        else:\n",
    "            item[\"mol\"] = mol\n",
    "\n",
    "        # Add other metadata to the item\n",
    "        item.update({\n",
    "            k: metadata[k] for k in [\"precursor_mz\"] # removed adduct due to a str error\n",
    "        })\n",
    "\n",
    "        if self.return_mol_freq:\n",
    "            item[\"mol_freq\"] = metadata[\"mol_freq\"]\n",
    "\n",
    "        if self.return_identifier:\n",
    "            item[\"identifier\"] = metadata[\"identifier\"]\n",
    "\n",
    "        # TODO: this should be refactored\n",
    "        for k, v in item.items():\n",
    "            if not isinstance(v, str):\n",
    "                item[k] = torch.as_tensor(v, dtype=self.dtype)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Init hyperparameters\n",
    "max_epochs = 1\n",
    "n_peaks = 60\n",
    "threshold = 0.9\n",
    "alpha = 0.75 # 0.25, 0.5, 0.75, 1 - found 0.25 as best\n",
    "gamma = 0.75 # 0.25, 0.5, 0.75, 1 - found 0.75 as best\n",
    "lr = 1e-5\n",
    "num_iterations = 1\n",
    "\n",
    "if DEBUG:\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 64\n",
    "\n",
    "for i in range (0, num_iterations):\n",
    "    # Load dataset\n",
    "    dataset = TestMassSpecDataset(\n",
    "        spec_transform=SpecTokenizer(n_peaks=n_peaks),\n",
    "        mol_transform = MolToPFASVector(),\n",
    "        pth='/teamspace/studios/this_studio/files/merged_massspec_nist20_with_fold.tsv'\n",
    "    )\n",
    "\n",
    "    # Init data module\n",
    "    data_module = MassSpecDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        split_pth=split_pth,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Init model\n",
    "    model = HalogenDetectorDreamsTest(\n",
    "        threshold=threshold,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr\n",
    "    )\n",
    "    # initialise the wandb logger and name your wandb project\n",
    "    wandb_logger = WandbLogger(project='PFASDetection-MergedMassSpecNIST20-HyperParam')\n",
    "    # add your batch size to the wandb config\n",
    "    wandb_logger.experiment.config[\"batch_size\"] = batch_size\n",
    "    wandb_logger.experiment.config[\"n_peaks\"] = n_peaks\n",
    "    wandb_logger.experiment.config[\"threshold\"] = threshold\n",
    "    wandb_logger.experiment.config[\"alpha\"] = alpha\n",
    "    wandb_logger.experiment.config[\"gamma\"] = gamma\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", max_epochs=max_epochs, logger=wandb_logger, val_check_interval=0.2)\n",
    "\n",
    "    # Validate before training\n",
    "    data_module.prepare_data()  # Explicit call needed for validate before fit\n",
    "    data_module.setup()  # Explicit call needed for validate before fit\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "    # # Train\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # [optional] finish the wandb run, necessary in notebooks\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Fluorine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dreams.utils.data import MSData\n",
    "from dreams.api import dreams_predictions, PreTrainedModel\n",
    "from dreams.models.heads.heads import BinClassificationHead\n",
    "from dreams.utils.io import append_to_stem\n",
    "\n",
    "\n",
    "def find_fluorine():\n",
    "\n",
    "    # in_pth = 'data/teo/<in_file>.mgf'  # or .mzML\n",
    "    # out_csv_pth = 'data/teo/<in_file>_f_preds.csv'\n",
    "\n",
    "    # in_pths = list(Path('/scratch/project/open-26-5/DreaMS/data/andrej/fluorine_dataset').glob('*.mzML'))\n",
    "    # model_ckpt_111k = '/scratch/project/open-26-5/DreaMS/dreams/HAS_F_1.0/Feb2025_8bs_5e-5lr_bce/epoch=30-step=111000.ckpt'\n",
    "    # model_ckpt_7k = '/scratch/project/open-26-5/DreaMS/dreams/HAS_F_1.0/Feb2025_8bs_5e-5lr_bce/epoch=1-step=7000.ckpt'\n",
    "\n",
    "    in_pth = Path('/teamspace/studios/this_studio/20250627_pa_flo_nmr_pos_norm.mzML')\n",
    "    # threshold = 0.9 model\n",
    "    model_ckpt = '/teamspace/studios/this_studio/HalogenDetection-FocalLoss-MergedMassSpecNIST20/opi4lx8s/checkpoints/epoch=0-step=8920.ckpt'\n",
    "\n",
    "    n_highest_peaks = 60\n",
    "\n",
    "    # Load model\n",
    "    model = HalogenDetectorDreamsTest.load_from_checkpoint(model_ckpt)\n",
    "    print(model)\n",
    "\n",
    "    print(f'Processing {in_pth}...')\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        msdata = MSData.load(in_pth, in_mem=True)\n",
    "    except ValueError as e:\n",
    "        print(f'Skipping {in_pth} because of {e}.')\n",
    "        return\n",
    "\n",
    "    # Compute fluorine probabilties\n",
    "    df = msdata.to_pandas()\n",
    "    \n",
    "    f_preds = dreams_predictions(\n",
    "        spectra=msdata,\n",
    "        model_ckpt=model,\n",
    "        n_highest_peaks=n_highest_peaks\n",
    "    )\n",
    "    df[f'F_preds'] = f_preds\n",
    "\n",
    "        # Store predictions\n",
    "    df.to_csv(append_to_stem(in_pth, 'F_preds').with_suffix('.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_fluorine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging NIST20 and MassSpecGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual file path\n",
    "file_path = '/teamspace/studios/this_studio/MassSpecGym/NIST20_MoNA_A_all_with_F_Murcko_split_MCE_test_minimum_cols.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "nist20_df = pd.read_pickle(file_path)\n",
    "\n",
    "# Check the result\n",
    "nist20_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist20_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the 'ID' starts with \"NIST20\"\n",
    "nist20_df = nist20_df[nist20_df['ID'].str.startswith(\"NIST20\")].copy()\n",
    "\n",
    "nist20_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from massspecgym.utils import load_massspecgym\n",
    "massspec_df = load_massspecgym().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "massspec_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: Preprocess nist20_df\n",
    "# -----------------------------\n",
    "nist20_df = nist20_df.copy()\n",
    "\n",
    "# Split 'PARSED PEAKS' into two columns\n",
    "nist20_df['mzs'] = nist20_df['PARSED PEAKS'].apply(lambda x: x[0])\n",
    "nist20_df['intensities'] = nist20_df['PARSED PEAKS'].apply(lambda x: x[1])\n",
    "\n",
    "# Build a MassSpec-compatible DataFrame from NIST20\n",
    "nist20_converted = pd.DataFrame({\n",
    "    'identifier': nist20_df['ID'],\n",
    "    'mzs': nist20_df['mzs'],\n",
    "    'intensities': nist20_df['intensities'],\n",
    "    'smiles': nist20_df['SMILES'],\n",
    "    'inchikey': None,  # Not available in NIST20\n",
    "    'formula': nist20_df['FORMULA'],\n",
    "    'precursor_formula': nist20_df['FORMULA'],  # Assume it's the same\n",
    "    'parent_mass': nist20_df['PRECURSOR M/Z'],  # Approximate\n",
    "    'precursor_mz': nist20_df['PRECURSOR M/Z'],\n",
    "    'adduct': '[M+H]+',\n",
    "    'instrument_type': None,\n",
    "    'collision_energy': None,\n",
    "    'fold': nist20_df['fold'],\n",
    "    'simulation_challenge': False  # NIST20 is real, not simulated\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Normalize MassSpec df\n",
    "# -----------------------------\n",
    "expected_columns = [\n",
    "    'identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula',\n",
    "    'parent_mass', 'precursor_mz', 'adduct', 'instrument_type',\n",
    "    'collision_energy', 'fold', 'simulation_challenge'\n",
    "]\n",
    "\n",
    "nist20_converted = nist20_converted[expected_columns]\n",
    "massspec_gym_df = massspec_df.copy()\n",
    "massspec_gym_df = massspec_gym_df[expected_columns]\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Merge the datasets\n",
    "# -----------------------------\n",
    "merged_df = pd.concat([massspec_gym_df, nist20_converted], ignore_index=True)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Save merged dataset\n",
    "# -----------------------------\n",
    "# Save as TSV\n",
    "merged_df.to_pickle('merged_massspec_nist20.pkl')\n",
    "\n",
    "# Check result\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Murcko Histogram Based Training/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual file path\n",
    "file_path = '/teamspace/studios/this_studio/files/merged_massspec_nist20.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "from rdkit import Chem\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dreams.algorithms.murcko_hist import murcko_hist\n",
    "from dreams.utils.data import MSData, evaluate_split\n",
    "from dreams.utils.plots import init_plotting\n",
    "from dreams.definitions import *\n",
    "tqdm.pandas()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = murcko_hist.murcko_hist(Chem.MolFromSmiles('O=C(O)[C@@H]1/N=C(\\SC1)c2sc3cc(O)ccc3n2'), show_mol_scaffold=True)\n",
    "print('Murcko histogram:', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = df.drop_duplicates(subset=[SMILES]).copy()  # Uniquify SMILES\n",
    "\n",
    "# Compute Murcko histograms\n",
    "df_us['MurckoHist'] = df_us[SMILES].progress_apply(\n",
    "    lambda x: murcko_hist.murcko_hist(Chem.MolFromSmiles(x))\n",
    ")\n",
    "\n",
    "# Convert dictionaries to strings for easier handling\n",
    "df_us['MurckoHistStr'] = df_us['MurckoHist'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num. unique SMILES:', df_us[SMILES].nunique(), 'Num. unique Murcko histograms:', df_us['MurckoHistStr'].nunique())\n",
    "print('Top 20 most common Murcko histograms:')\n",
    "df_us['MurckoHistStr'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by MurckoHistStr and aggregate\n",
    "df_gb = df_us.groupby('MurckoHistStr').agg(\n",
    "    count=(SMILES, 'count'),\n",
    "    smiles_list=(SMILES, list)\n",
    ").reset_index()\n",
    "\n",
    "# Convert MurckoHistStr to MurckoHist\n",
    "df_gb['MurckoHist'] = df_gb['MurckoHistStr'].apply(eval)\n",
    "\n",
    "# Sort by 'n' in descending order and reset index\n",
    "df_gb = df_gb.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_i = len(df_gb) // 2\n",
    "cum_val_mols = 0\n",
    "val_mols_frac = 0.15  # Approximately 15% of the molecules go to validation set\n",
    "val_idx, train_idx = [], []\n",
    "\n",
    "# Iterate from median to start, assigning molecules to train or val sets\n",
    "for i in range(median_i, -1, -1):\n",
    "    current_hist = df_gb.iloc[i]['MurckoHist']\n",
    "    is_val_subhist = any(\n",
    "        murcko_hist.are_sub_hists(current_hist, df_gb.iloc[j]['MurckoHist'], k=3, d=4)\n",
    "        for j in val_idx\n",
    "    )\n",
    "\n",
    "    if is_val_subhist:\n",
    "        train_idx.append(i)\n",
    "    else:\n",
    "        if cum_val_mols / len(df_us) <= val_mols_frac:\n",
    "            cum_val_mols += df_gb.iloc[i]['count']\n",
    "            val_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "\n",
    "# Add remaining indices to train set\n",
    "train_idx.extend(range(median_i + 1, len(df_gb)))\n",
    "assert(len(train_idx) + len(val_idx) == len(df_gb))\n",
    "\n",
    "# Map SMILES to their assigned fold\n",
    "smiles_to_fold = {}\n",
    "for i, row in df_gb.iterrows():\n",
    "    fold = 'val' if i in val_idx else 'train'\n",
    "    for smiles in row['smiles_list']:\n",
    "        smiles_to_fold[smiles] = fold\n",
    "df[FOLD] = df[SMILES].map(smiles_to_fold)\n",
    "\n",
    "# Display fold distributions\n",
    "print('Distribution of spectra:')\n",
    "display(df[FOLD].value_counts(normalize=True))\n",
    "print('Distribution of SMILES:')\n",
    "display(df.drop_duplicates(subset=[SMILES])[FOLD].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = evaluate_split(df, n_workers=4)\n",
    "init_plotting(figsize=(3, 3))\n",
    "sns.histplot(eval_res['val'], bins=100)\n",
    "plt.xlabel('Max Tanimoto similarity to training set')\n",
    "plt.ylabel('Num. validation set molecules')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print('Num. unique inchikey:', df['inchikey'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df.groupby('inchikey').agg(\n",
    "    count=(SMILES, 'count')\n",
    ").reset_index()\n",
    "\n",
    "df_t = df_t.sort_values(by='count', ascending=False).reset_index()\n",
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_peaks(mzs, intensities):\n",
    "    # Filter out zero values in either mz or intensity\n",
    "    filtered = [(mz, inten) for mz, inten in zip(mzs, intensities) if mz != 0 and inten != 0]\n",
    "    \n",
    "    if not filtered:\n",
    "        return [], []\n",
    "    \n",
    "    # Sort by mz\n",
    "    filtered.sort(key=lambda x: x[0])\n",
    "    \n",
    "    mzs_clean, intensities_clean = zip(*filtered)\n",
    "    return list(mzs_clean), list(intensities_clean)\n",
    "\n",
    "# Apply to entire DataFrame\n",
    "df[['mzs', 'intensities']] = df.apply(\n",
    "    lambda row: pd.Series(remove_zero_peaks(row['mzs'], row['intensities'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mzs and intensities into a comma separated list for serializing to disk\n",
    "df['mzs'] = df['mzs'].apply(lambda x: ','.join(map(str, x)))\n",
    "df['intensities'] = df['intensities'].apply(lambda x: ','.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('merged_massspec_nist20_with_fold.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv('/teamspace/studios/this_studio/files/merged_massspec_nist20_with_fold.tsv', sep='\\t')\n",
    "df_t.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect PFAS in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "# SMARTS for –CF3 and –CF2– groups (saturated and fully fluorinated)\n",
    "cf3_smarts = '[CX4](F)(F)F'       # –CF3\n",
    "cf2_smarts = '[CX4H0](F)(F)'      # –CF2– (not terminal, excludes CF3)\n",
    "\n",
    "cf3_pattern = Chem.MolFromSmarts(cf3_smarts)\n",
    "cf2_pattern = Chem.MolFromSmarts(cf2_smarts)\n",
    "\n",
    "def is_pfas_oecd(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return False\n",
    "\n",
    "        if mol.HasSubstructMatch(cf3_pattern) or mol.HasSubstructMatch(cf2_pattern):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception:\n",
    "        return 'Error'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "# Example: Load your dataframe\n",
    "df = pd.read_csv('/teamspace/studios/this_studio/files/merged_massspec_nist20_with_fold.tsv', sep='\\t')\n",
    "\n",
    "# Apply the function\n",
    "df['is_PFAS'] = df['smiles'].apply(is_pfas_oecd)\n",
    "\n",
    "# View how many were identified\n",
    "print(f\"Identified {df['is_PFAS'].sum()} potential PFAS compounds out of {len(df)} total.\")\n",
    "\n",
    "# Optionally: get only the PFAS rows\n",
    "pfas_df = df[df['is_PFAS']]\n",
    "\n",
    "#df.to_csv('merged_massspec_nist20_with_pfas_fold.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "import random as r\n",
    "import pandas as pd\n",
    "\n",
    "# read pfas dataset\n",
    "pfas_df = pd.read_csv('/teamspace/studios/this_studio/files/pfas_only_records.tsv', sep='\\t')\n",
    "unique_pfas_train = pfas_df[pfas_df['fold'] == 'train']['smiles'].unique()\n",
    "unique_pfas_val = pfas_df[pfas_df['fold'] == 'val']['smiles'].unique()\n",
    "print(f\"Train PFAS = {len(unique_pfas_train)}, Val PFAS = {len(unique_pfas_val)}\")\n",
    "\n",
    "print(f\"Drawing a random molecule from train\")\n",
    "smiles_list = unique_pfas_train.tolist()\n",
    "m = Chem.MolFromSmiles(r.choice(smiles_list))\n",
    "img = Draw.MolToImage(m)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfas_df.to_csv('pfas_only_records.csv', sep='\\t')\n",
    "#only_val_df = pfas_df[pfas_df['fold'] == 'val']\n",
    "#only_val_df.to_csv('pfas_only_records_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "pfas_dataset = TestMassSpecDataset(\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform = MolToHalogensVector(),\n",
    "    pth='/teamspace/studios/this_studio/pfas_only_records.tsv'\n",
    ")\n",
    "\n",
    "print(len(pfas_dataset))\n",
    "\n",
    "# Init data module\n",
    "pfas_data_module = MassSpecDataModule(\n",
    "    dataset=pfas_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=1\n",
    ")\n",
    "pfas_data_module.setup()\n",
    "\n",
    "ckpt_path = '/teamspace/studios/this_studio/HalogenDetection-FocalLoss-MergedMassSpecNIST20/opi4lx8s/checkpoints/epoch=0-step=8920.ckpt'\n",
    "model = HalogenDetectorDreamsTest.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "trainer = Trainer(accelerator=\"auto\", devices=\"auto\", max_epochs=1)\n",
    "trainer.validate(model=model, datamodule=pfas_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys:\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n",
      "Model state_dict {'lr': 1e-05, 'weight_decay': 0.0, 'log_only_loss_at_stages': (<Stage.TRAIN: 'train'>,), 'no_mces_metrics_at_stages': (<Stage.VAL: 'val'>,), 'bootstrap_metrics': False, 'df_test_path': None, 'alpha': 0.25, 'gamma': 0.75, 'batch_size': 64, 'threshold': 0.9}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Fluorine Model\n",
    "# threshold - 0.75\n",
    "## /teamspace/studios/this_studio/HalogenDetection-FocalLoss-MergedMassSpecNIST20/6qcft1pp\n",
    "# threshold - 0.9\n",
    "## /teamspace/studios/this_studio/HalogenDetection-FocalLoss-MergedMassSpecNIST20/opi4lx8s/checkpoints/epoch=0-step=8920.ckpt\n",
    "## opi4lx8s - threshold - 0.9\n",
    "\n",
    "\n",
    "# PFAS Model\n",
    "# threshold - 0.9\n",
    "## /teamspace/studios/this_studio/PFASDetection-FocalLoss-MergedMassSpecNIST20/31hkfun1/checkpoints/epoch=0-step=8920.ckpt\n",
    "## /teamspace/studios/this_studio/PFASDetection-FocalLoss-MergedMassSpecNIST20/kxfsf9c5/checkpoints/epoch=0-step=8920.ckpt\n",
    "## /teamspace/studios/this_studio/PFASDetection-FocalLoss-MergedMassSpecNIST20/nrw1m4b9/checkpoints/epoch=0-step=8920.ckpt\n",
    "\n",
    "# Path to your checkpoint file\n",
    "ckpt_path = '/teamspace/studios/this_studio/HalogenDetection-FocalLoss-MergedMassSpecNIST20/opi4lx8s/checkpoints/epoch=0-step=8920.ckpt'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "# Print available metadata keys\n",
    "print(\"Checkpoint keys:\")\n",
    "print(checkpoint.keys())\n",
    "\n",
    "# Optionally, display specific metadata if available\n",
    "if 'state_dict' in checkpoint:\n",
    "    print(f\"Model state_dict {checkpoint['hyper_parameters']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file merged_massspec_nist20.pkl\n",
    "# do EDA on nist20 vs masspec data and look at the number of unique inchikeys and smiles\n",
    "import pandas as pd\n",
    "df_unique = pd.read_pickle('/teamspace/studios/this_studio/files/merged_massspec_nist20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_massspec = df_unique[df_unique[\"identifier\"].str.startswith(\"MassSpecGym\")]\n",
    "df_nist = df_unique[df_unique[\"identifier\"].str.startswith(\"NIST20\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inchi_ms = df_massspec[\"inchikey\"].nunique()\n",
    "num_inchi_nist = df_nist[\"inchikey\"].nunique()\n",
    "num_smil_ms = df_massspec[\"smiles\"].nunique()\n",
    "num_smil_nist = df_nist[\"smiles\"].nunique()\n",
    "\n",
    "print(f\"num_inchi_ms = {num_inchi_ms}, num_inchi_nist = {num_inchi_nist}, num_smil_ms = {num_smil_ms}, num_smil_nist = {num_smil_nist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import urllib\n",
    "import json\n",
    "import time\n",
    "import ase\n",
    "import rdkit\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from rdkit import DataStructs, RDLogger\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import rdchem, Draw, rdMolDescriptors, QED, Crippen, Lipinski\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem.MACCSkeys import GenMACCSKeys\n",
    "from rdkit.Contrib.SA_Score import sascorer\n",
    "from rdkit.Chem.Descriptors import ExactMolWt\n",
    "from collections import defaultdict\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import dreams.utils.misc as utils\n",
    "\n",
    "\n",
    "def show_mols(mols, legends='new_indices', smiles_in=None, svg=False, sort_by_legend=False, max_mols=500,\n",
    "              legend_float_decimals=4, mols_per_row=6, save_pth: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Returns svg image representing a grid of skeletal structures of the given molecules\n",
    "\n",
    "    :param mols: list of rdkit molecules\n",
    "    :param legends: list of labels for each molecule, length must be equal to the length of mols. \n",
    "                   Can be 'new_indices' for default numbering, 'masses' for molecular weights,\n",
    "                   or a list of custom labels\n",
    "    :param smiles_in: True - SMILES inputs, False - RDKit mols, None - determine automatically\n",
    "    :param svg: True - return svg image, False - return png image\n",
    "    :param sort_by_legend: True - sort molecules by legend values\n",
    "    :param max_mols: maximum number of molecules to show\n",
    "    :param legend_float_decimals: number of decimal places to show for float legends\n",
    "    :param mols_per_row: number of molecules per row to show\n",
    "    :param save_pth: path to save the .svg image to\n",
    "    \"\"\"\n",
    "    disable_rdkit_log()\n",
    "\n",
    "    if smiles_in is None:\n",
    "        smiles_in = all(isinstance(e, str) for e in mols)\n",
    "\n",
    "    if smiles_in:\n",
    "        mols = [Chem.MolFromSmiles(e) for e in mols]\n",
    "\n",
    "    if isinstance(legends, str):\n",
    "        if legends == 'new_indices':\n",
    "            legends = list(range(len(mols)))\n",
    "        elif legends == 'masses':\n",
    "            legends = [ExactMolWt(m) for m in mols]\n",
    "    elif callable(legends):\n",
    "        legends = [legends(e) for e in mols]\n",
    "    elif isinstance(legends, (list, np.ndarray, pd.Series)):\n",
    "        legends = [str(l) for l in legends]\n",
    "    else:\n",
    "        raise ValueError(f'Invalid legends type: {type(legends)}. Must be a list, numpy array, pandas series or'\n",
    "                         '\"new_indices\" or \"masses\".')\n",
    "\n",
    "    if sort_by_legend:\n",
    "        idx = np.argsort(legends).tolist()\n",
    "        legends = [legends[i] for i in idx]\n",
    "        mols = [mols[i] for i in idx]\n",
    "\n",
    "    legends = [f'{l:.{legend_float_decimals}f}' if isinstance(l, float) else str(l) for l in legends]\n",
    "\n",
    "    img = Draw.MolsToGridImage(mols, maxMols=max_mols, legends=legends, molsPerRow=min(max_mols, mols_per_row),\n",
    "                         useSVG=svg, returnPNG=False)\n",
    "\n",
    "    if save_pth:\n",
    "        with open(save_pth, 'w') as f:\n",
    "            f.write(img.data)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def mol_to_formula(mol, as_dict=False):\n",
    "    formula = rdMolDescriptors.CalcMolFormula(mol)\n",
    "    return formula_to_dict(formula) if as_dict else formula\n",
    "\n",
    "\n",
    "def smiles_to_formula(s, as_dict=False, invalid_mol_smiles=''):\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if not mol and invalid_mol_smiles is not None:\n",
    "        f = invalid_mol_smiles\n",
    "    else:\n",
    "        f = rdMolDescriptors.CalcMolFormula(mol)\n",
    "    if as_dict:\n",
    "        f = formula_to_dict(f)\n",
    "    return f\n",
    "\n",
    "\n",
    "class MolPropertyCalculator:\n",
    "    def __init__(self):\n",
    "        # Estimates of min and max values from the training part of MoNA and NIST20 Murcko histograms split\n",
    "        self.min_maxs = {\n",
    "            'AtomicLogP': {'min': -13.054800000000025, 'max': 26.849200000000053},\n",
    "            'NumHAcceptors': {'min': 0.0, 'max': 36.0},\n",
    "            'NumHDonors': {'min': 0.0, 'max': 20.0},\n",
    "            'PolarSurfaceArea': {'min': 0.0, 'max': 585.0300000000002},\n",
    "            'NumRotatableBonds': {'min': 0.0, 'max': 68.0},\n",
    "            'NumAromaticRings': {'min': 0.0, 'max': 8.0},\n",
    "            'NumAliphaticRings': {'min': 0.0, 'max': 22.0},\n",
    "            'FractionCSP3': {'min': 0.0, 'max': 1.0},\n",
    "            'QED': {'min': 0.0, 'max': 1.0},  # 'QED': {'min': 0.008950206972239864, 'max': 0.9479380820623227},\n",
    "            'SyntheticAccessibility': {'min': 1.0, 'max': 10.0},  # 'SyntheticAccessibility': {'min': 1.0549172379947862, 'max': 8.043981630210263},\n",
    "            'BertzComplexity': {'min': 2.7548875021634682, 'max': 3748.669248605835}\n",
    "        }\n",
    "        self.prop_names = list(self.min_maxs.keys())\n",
    "\n",
    "    def mol_to_props(self, mol, min_max_norm=False):\n",
    "        props = {\n",
    "            'AtomicLogP': Crippen.MolLogP(mol),\n",
    "            'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
    "            'NumHDonors': Lipinski.NumHDonors(mol),\n",
    "            'PolarSurfaceArea': rdMolDescriptors.CalcTPSA(mol),\n",
    "            'NumRotatableBonds': Lipinski.NumRotatableBonds(mol),\n",
    "            'NumAromaticRings': Lipinski.NumAromaticRings(mol),\n",
    "            'NumAliphaticRings': Lipinski.NumAliphaticRings(mol),\n",
    "            'FractionCSP3': Lipinski.FractionCSP3(mol),\n",
    "            'QED': QED.qed(mol),\n",
    "            'SyntheticAccessibility': sascorer.calculateScore(mol),\n",
    "            'BertzComplexity': rdkit.Chem.GraphDescriptors.BertzCT(mol)\n",
    "        }\n",
    "        if min_max_norm:\n",
    "            props = self.normalize_props(props)\n",
    "        return props\n",
    "\n",
    "    def normalize_prop(self, prop, prop_name):\n",
    "        return (prop - self.min_maxs[prop_name]['min']) / (self.min_maxs[prop_name]['max'] - self.min_maxs[prop_name]['min'])\n",
    "\n",
    "    def denormalize_prop(self, prop, prop_name, do_not_add_min=False):\n",
    "        res = prop * (self.min_maxs[prop_name]['max'] - self.min_maxs[prop_name]['min'])\n",
    "        if not do_not_add_min:\n",
    "            res = res + self.min_maxs[prop_name]['min']\n",
    "        return res\n",
    "\n",
    "    def normalize_props(self, props):\n",
    "        return {k: self.normalize_prop(v, k) for k, v in props.items()}\n",
    "\n",
    "    def denormalize_props(self, props):\n",
    "        return {k: self.denormalize_prop(v, k) for k, v in props.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prop_names)\n",
    "\n",
    "\n",
    "def formula_to_dict(formula):\n",
    "    \"\"\"\n",
    "    Transforms chemical formula string to dictionary mapping elements to their frequencies\n",
    "    e.g. 'C15H24' -> {'C': 15, 'H': 24}\n",
    "    \"\"\"\n",
    "    elem_count = defaultdict(int)\n",
    "    #try:\n",
    "    formula = formula.replace('+', '').replace('-', '').replace('[', '').replace(']', '')\n",
    "    formula_counts = ase.formula.Formula(formula)\n",
    "    formula_counts = formula_counts.count().items()\n",
    "    for k, v in formula_counts:\n",
    "        elem_count[k] += v\n",
    "    #except Exception as e:\n",
    "    #    print(f'Invalid formula: {formula} ({e.__class__.__name__})')\n",
    "\n",
    "    return elem_count\n",
    "\n",
    "\n",
    "def rdkit_fp(mol, fp_size=4096):\n",
    "    \"\"\"Default RDKit fingerprint.\"\"\"\n",
    "    return Chem.RDKFingerprint(mol, fpSize=fp_size)\n",
    "\n",
    "\n",
    "def tanimoto_sim(fp1, fp2):\n",
    "    \"\"\"Default RDKit Tanimoto distance.\"\"\"\n",
    "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "\n",
    "def rdkit_mol_sim(m1, m2, fp_size=4096):\n",
    "    \"\"\"Default RDKit Tanimoto distance on default RDKit fingerprint.\"\"\"\n",
    "    return tanimoto_sim(rdkit_fp(m1, fp_size=fp_size), rdkit_fp(m2, fp_size=fp_size))\n",
    "\n",
    "\n",
    "def rdkit_smiles_sim(s1, s2, fp_size=4096):\n",
    "    \"\"\"Default RDKit Tanimoto distance on default RDKit fingerprint.\"\"\"\n",
    "    return rdkit_mol_sim(Chem.MolFromSmiles(s1), Chem.MolFromSmiles(s2), fp_size=fp_size)\n",
    "\n",
    "\n",
    "def morgan_fp(mol, binary=True, fp_size=4096, radius=2, as_numpy=True):\n",
    "    if binary:\n",
    "        fp = Chem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=fp_size)\n",
    "    else:\n",
    "        fp = Chem.GetHashedMorganFingerprint(mol, radius=radius, nBits=fp_size)\n",
    "\n",
    "    if as_numpy:\n",
    "        return rdkit_fp_to_np(fp)\n",
    "    return fp\n",
    "\n",
    "\n",
    "def maccs_fp(mol, as_numpy=True):\n",
    "    \"\"\"\n",
    "    NOTE: Since indexing of MACCS keys starts from 1, when converting to numpy array with `as_numpy`, the first element\n",
    "          is removed, so the resulting array has 166 elements instead of 167.\n",
    "    \"\"\"\n",
    "    fp = GenMACCSKeys(mol)\n",
    "    if as_numpy:\n",
    "        return rdkit_fp_to_np(fp)[1:]\n",
    "    return fp\n",
    "\n",
    "\n",
    "def fp_func_from_str(s):\n",
    "    \"\"\"\n",
    "    :param s: E.g. \"fp_rdkit_2048\", \"fp_rdkit_2048\" or \"fp_maccs_166\".\n",
    "    \"\"\"\n",
    "    _, fp_type, n_bits = s.split('_')\n",
    "    n_bits = int(n_bits)\n",
    "    if fp_type == 'rdkit':\n",
    "        return lambda mol: np.array(rdkit_fp(mol, fp_size=n_bits), dtype=float)\n",
    "    elif fp_type == 'morgan':\n",
    "        return lambda mol: morgan_fp(mol, fp_size=n_bits).astype(float, copy=False)\n",
    "    elif fp_type == 'maccs':\n",
    "        return lambda mol: maccs_fp(mol).astype(float, copy=False)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid fingerprint function name: \"{s}\".')\n",
    "\n",
    "\n",
    "def morgan_mol_sim(m1, m2, fp_size=4096, radius=2):\n",
    "    return tanimoto_sim(\n",
    "        morgan_fp(m1, fp_size=fp_size, radius=radius, as_numpy=False),\n",
    "        morgan_fp(m2, fp_size=fp_size, radius=radius, as_numpy=False)\n",
    "    )\n",
    "\n",
    "\n",
    "def morgan_smiles_sim(s1, s2, fp_size=4096, radius=2):\n",
    "    return morgan_mol_sim(Chem.MolFromSmiles(s1), Chem.MolFromSmiles(s2), fp_size=fp_size, radius=radius)\n",
    "\n",
    "\n",
    "def rdkit_fp_to_np(fp):\n",
    "    fp_np = np.zeros((0,), dtype=np.int32)\n",
    "    DataStructs.ConvertToNumpyArray(fp, fp_np)\n",
    "    return fp_np\n",
    "\n",
    "\n",
    "def np_to_rdkit_fp(fp):\n",
    "    fp = fp.round().astype(int, copy=False)\n",
    "    bitstring = ''.join(fp.astype(str))\n",
    "    return DataStructs.cDataStructs.CreateFromBitString(bitstring)\n",
    "\n",
    "\n",
    "def mol_to_inchi14(mol: Chem.Mol):\n",
    "    return Chem.MolToInchiKey(mol).split('-')[0]\n",
    "\n",
    "\n",
    "def smiles_to_inchi14(s):\n",
    "    return mol_to_inchi14(Chem.MolFromSmiles(s))\n",
    "\n",
    "\n",
    "def generate_fragments(mol: Chem.Mol, max_cuts: int = None):\n",
    "    \"\"\"\n",
    "    Generates all possible fragments of a molecule up to a certain number of bond cuts or without the restriction if\n",
    "    `max_cuts` is not specified.\n",
    "\n",
    "    :param mol: an RDKit molecule object\n",
    "    :param max_cuts: the maximum number of bonds to cut\n",
    "    :return a set of RDKit Mol objects representing all possible fragments\n",
    "    \"\"\"\n",
    "\n",
    "    bonds = mol.GetBonds()\n",
    "    # bonds = [bond for bond in bonds if bond.GetBondType() in [rdchem.BondType.SINGLE, rdchem.BondType.DOUBLE]]\n",
    "    fragments = set()\n",
    "    for i in range(1, len(bonds) + 1):\n",
    "\n",
    "        if max_cuts and i > max_cuts:\n",
    "            break\n",
    "\n",
    "        for combination in itertools.combinations(bonds, i):\n",
    "            new_mol = rdchem.RWMol(mol)\n",
    "            for bond in combination:\n",
    "                new_mol.RemoveBond(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
    "\n",
    "            # Update properties such as ring membership after changing the molecule's structure.\n",
    "            for fragment in Chem.GetMolFrags(new_mol, asMols=True, sanitizeFrags=False):\n",
    "                fragments.add(Chem.MolToSmiles(fragment))\n",
    "\n",
    "    fragments = [Chem.MolFromSmiles(f) for f in fragments]\n",
    "    return [f for f in fragments if f is not None]\n",
    "\n",
    "\n",
    "def generate_spectrum(mol: Chem.Mol, prec_mz: float = None, fragments: List = None, max_cuts: int = None):\n",
    "    \"\"\"\n",
    "    Generates an MS/MS spectrum by exhaustively simulating the m/z values of theoretical fragments of a given molecule.\n",
    "    The algorithm is very simplistic since it considers only subgraph-like fragments, does not consider isotopes, etc.\n",
    "\n",
    "    :param mol: An RDKit molecule object.\n",
    "    :param prec_mz: The m/z value of a molecule. If not specified, it is calculated as the sum of the\n",
    "                    exact molecular weight of the molecule and 1.\n",
    "    :param fragments: A list of RDKit Mol objects representing pre-generated fragments of the molecule. If not specified,\n",
    "                     the function will generate the fragments automatically.\n",
    "    :param max_cuts: The maximum number of bonds to cut when generating fragments. If not specified, all possible\n",
    "                     fragments will be generated without any restriction on the number of cuts.\n",
    "    :return: A spectrum represented as a numpy array with two columns: m/z values and their respective intensities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Simulate the m/z of \"protonated adduct\"\n",
    "    if not prec_mz:\n",
    "        prec_mz = ExactMolWt(mol) + 1\n",
    "\n",
    "    # Fragment molecule\n",
    "    if not fragments:\n",
    "        fragments = generate_fragments(mol, max_cuts=max_cuts)\n",
    "\n",
    "    # Simulate spectrum\n",
    "    masses = np.round(np.array([prec_mz - ExactMolWt(f) for f in fragments]))\n",
    "    ins, mzs = np.histogram(masses, bins=np.arange(0, np.ceil(max(masses)), 1))\n",
    "    spec = np.stack([mzs[1:], ins]).T\n",
    "\n",
    "    return spec\n",
    "\n",
    "\n",
    "def closest_mz_frags(query_mz, frags, n=1, mass_shift=1, return_masses=False, print_masses=True):\n",
    "    masses = [ExactMolWt(f) + mass_shift for f in frags]\n",
    "    idx = utils.get_closest_values(masses, query_mz, n=n, return_idx=True)\n",
    "    frags, masses = [frags[i] for i in idx], [masses[i] for i in idx]\n",
    "    if n == 1:\n",
    "        frags, masses = frags[0], masses[0]\n",
    "    if print_masses:\n",
    "        print(masses)\n",
    "    if return_masses:\n",
    "        return frags, masses\n",
    "    return frags\n",
    "\n",
    "\n",
    "def disable_rdkit_log():\n",
    "    lg = RDLogger.logger()\n",
    "    lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "def np_classify(smiles: List[str], progress_bar=True, sleep_each_n_requests=100):\n",
    "    np_classes = []\n",
    "    for i, s in enumerate(tqdm(smiles) if progress_bar else smiles):\n",
    "        if i % sleep_each_n_requests == 0 and i > 0:\n",
    "            time.sleep(1)\n",
    "        print(s)\n",
    "        with urllib.request.urlopen(f'https://npclassifier.ucsd.edu/classify?smiles={urllib.parse.quote(s)}') as url:\n",
    "            res = json.load(url)\n",
    "            for k in list(res.keys()):\n",
    "                if 'fp' in k:\n",
    "                    res.pop(k)\n",
    "            np_classes.append(res)\n",
    "    return np_classes\n",
    "\n",
    "\n",
    "def mol_to_img_str(mol, svg_size=200):\n",
    "    \"\"\"\n",
    "    Supposed to be used with `pyvis` for showing molecule images as graph nodes.\n",
    "    \"\"\"\n",
    "    buffered = BytesIO()\n",
    "    d2d = rdMolDraw2D.MolDraw2DSVG(svg_size, svg_size)\n",
    "    opts = d2d.drawOptions()\n",
    "    opts.clearBackground = False\n",
    "    d2d.DrawMolecule(mol)\n",
    "    d2d.FinishDrawing()\n",
    "    img_str = d2d.GetDrawingText()\n",
    "    buffered.write(str.encode(img_str))\n",
    "    img_str = base64.b64encode(buffered.getvalue())\n",
    "    img_str = f\"data:image/svg+xml;base64,{repr(img_str)[2:-1]}\"\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def formula_is_carbohydrate(formula):\n",
    "    return set(formula.keys()) <= {'C', 'H', 'O'}\n",
    "\n",
    "\n",
    "def formula_is_halogenated(formula):\n",
    "    return sum([(formula[e] if e in formula else 0) for e in ['F', 'Cl', 'Br', 'I']]) > 0\n",
    "\n",
    "\n",
    "def formula_type(f):\n",
    "    if isinstance(f, str):\n",
    "        f = formula_to_dict(f)\n",
    "\n",
    "    if not f:\n",
    "        return 'No formula'\n",
    "    elif formula_is_carbohydrate(f):\n",
    "        return 'Carbohydrate'\n",
    "    elif set(f.keys()) <= {'C', 'H', 'O', 'N'}:\n",
    "        return 'Carbohydrate with nitrogen'\n",
    "    elif set(f.keys()) <= {'C', 'H', 'O', 'N', 'S'} and 'N' in f and 'S' in f:\n",
    "        return 'Carbohydrate with nitrogen and sulfur'\n",
    "    elif formula_is_halogenated(f):\n",
    "        return 'Compound with halogens'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "def get_mol_mass(mol):\n",
    "    return ExactMolWt(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_massspec = df_unique[df_unique[\"identifier\"].str.startswith(\"MassSpecGym\")]\n",
    "#df_nist = df_unique[df_unique[\"identifier\"].str.startswith(\"NIST20\")]\n",
    "#df_unique = pd.read_pickle('merged_massspec_nist20.pkl')\n",
    "\n",
    "df_nist['inchikey'] = df_nist['smiles'].apply(smiles_to_inchi14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inchi_ms = df_massspec[\"inchikey\"].nunique()\n",
    "num_inchi_nist = df_nist[\"inchikey\"].nunique()\n",
    "print(\"NIST unique # inchikeys: \" + str(num_inchi_nist))\n",
    "print(\"MassSpecGym unique # inchikeys: \" + str(num_inchi_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records columns: ['Unnamed: 0.1', 'Unnamed: 0', 'identifier', 'mzs', 'intensities', 'smiles', 'inchikey', 'formula', 'precursor_formula', 'parent_mass', 'precursor_mz', 'adduct', 'instrument_type', 'collision_energy', 'fold', 'simulation_challenge', 'is_PFAS']\n",
      "Suspects columns: ['SUSPECTID', 'CHEMICAL_NAME', 'INCHI', 'SMILES', 'INCHIKEY', 'FIXEDINCHI', 'FORMULA', 'FIXEDMASS', 'NETCHARGE', 'LOCAL_POSITIVE', 'LOCAL_NEGATIVE', 'DOI', 'CITATION_TYPE', 'ADDITIONAL', 'INSPECTEDBY']\n",
      "Total in PFAS_Suspect_List: 4964\n",
      "Total in pfas_only_records: 1469\n",
      "Overlapping SMILES: 4\n",
      "C(COP(=O)(O)O)C(C(C(C(C(C(C(C(F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F\n",
      "C(COP(=O)(O)O)C(C(C(C(C(C(F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F\n",
      "C(COP(=O)(O)OCCC(C(C(C(C(C(C(C(F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)C(C(C(C(C(C(C(C(F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F\n",
      "C(COP(=O)(O)OCCC(C(C(C(C(C(F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F)C(C(C(C(C(C(F)(F)F)(F)F)(F)F)(F)F)(F)F)(F)F\n"
     ]
    }
   ],
   "source": [
    "# Overlap our PFAS training and PFAS suspect list from data.gov\n",
    "import pandas as pd\n",
    "\n",
    "# Load both TSV files\n",
    "df_records = pd.read_csv(\"/teamspace/studios/this_studio/files/pfas_only_records.tsv\", sep='\\t')\n",
    "df_suspects = pd.read_csv(\"/teamspace/studios/this_studio/files/PFAS_suspect_list_data_gov.tsv\", sep='\\t')\n",
    "\n",
    "# Preview column names\n",
    "print(\"Records columns:\", df_records.columns.tolist())\n",
    "print(\"Suspects columns:\", df_suspects.columns.tolist())\n",
    "\n",
    "# Standardize column names\n",
    "smiles_records = df_records[df_records['fold'] == 'train']['smiles'].dropna().str.strip().unique()\n",
    "smiles_suspects = df_suspects['SMILES'].dropna().str.strip().unique()\n",
    "\n",
    "# Convert to sets for comparison\n",
    "set_records = set(smiles_records)\n",
    "set_suspects = set(smiles_suspects)\n",
    "\n",
    "# Find overlap\n",
    "overlap = set_suspects.intersection(set_records)\n",
    "\n",
    "# Report results\n",
    "print(f\"Total in PFAS_Suspect_List: {len(set_suspects)}\")\n",
    "print(f\"Total in pfas_only_records: {len(set_records)}\")\n",
    "print(f\"Overlapping SMILES: {len(overlap)}\")\n",
    "\n",
    "for smile in sorted(overlap):\n",
    "        print(smile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
