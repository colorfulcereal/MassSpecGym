{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install \n",
    "\n",
    "# Download this repository\n",
    "#!git clone https://github.com/pluskal-lab/DreaMS.git\n",
    "#!cd DreaMS\n",
    "\n",
    "# Create conda environment\n",
    "#!conda update -n base -c defaults conda\n",
    "#!conda create -n dreams python==3.11.0 --yes\n",
    "#!conda init\n",
    "#!conda activate dreams\n",
    "\n",
    "# Install DreaMS\n",
    "#%pip install -e ./DreaMS\n",
    "\n",
    "#%pip install pytorch-lightning\n",
    "#!git clone https://github.com/colorfulcereal/MassSpecGym\n",
    "#%pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e /teamspace/studios/this_studio/MassSpecGym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MassSpecGym dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from massspecgym.utils import load_massspecgym\n",
    "df = load_massspecgym()\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['MassSpecGymID0013583']['mzs'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MS/MS EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot spectra\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Function to plot histogram of mzs and intensities\n",
    "def plot_spectrum(mzs, intensities):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.stem(mzs, intensities, basefmt='-')\n",
    "    plt.title('Mass Spectrum')\n",
    "    plt.xlabel('m/z')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.show()\n",
    "\n",
    "# Plot a random spectrum\n",
    "random_index = np.random.randint(0, len(df))\n",
    "print(random_index)\n",
    "mzs = df.iloc[random_index]['mzs']\n",
    "intensities = df.iloc[random_index]['intensities']\n",
    "plot_spectrum(mzs, intensities)\n",
    "\n",
    "# Plot multiple spectra\n",
    "num_spectra = 5\n",
    "random_indices = np.random.randint(0, len(df), num_spectra)\n",
    "fig, axs = plt.subplots(nrows=num_spectra, ncols=1, figsize=(8, 6*num_spectra))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    mzs = df.iloc[idx]['mzs']\n",
    "    intensities = df.iloc[idx]['intensities']\n",
    "    axs[i].stem(mzs, intensities, basefmt='-')\n",
    "    axs[i].set_title(f'Spectrum {idx}')\n",
    "    axs[i].set_xlabel('m/z')\n",
    "    axs[i].set_ylabel('Intensity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Halogen symbols\n",
    "halogens = ['F', 'Cl', 'Br', 'I', 'At', 'Ts']\n",
    "\n",
    "# Initialize halogen count dictionary\n",
    "halogen_counts = {halogen: 0 for halogen in halogens}\n",
    "\n",
    "# Count mass spectra containing each halogen\n",
    "for index, row in df.iterrows():\n",
    "    for halogen in halogens:\n",
    "        if halogen in row['precursor_formula'] or halogen in row['formula']:\n",
    "            if halogen == 'F':\n",
    "                print(index)\n",
    "            halogen_counts[halogen] += 1\n",
    "\n",
    "# Calculate percentages\n",
    "halogen_percentages = {halogen: (count/len(df))*100 for halogen, count in halogen_counts.items()}\n",
    "\n",
    "# Print results\n",
    "print(\"Halogen Percentages:\")\n",
    "for halogen, percentage in halogen_percentages.items():\n",
    "    print(f\"{halogen}: {percentage:.2f}%\")\n",
    "\n",
    "for halogen, count in halogen_counts.items():\n",
    "    print(f\"{halogen}: {count}\")\n",
    "\n",
    "# Plot (%)\n",
    "# plt.bar(halogen_percentages.keys(), halogen_percentages.values())\n",
    "# plt.xlabel('Halogen')\n",
    "# plt.ylabel('Percentage (%)')\n",
    "# plt.title('Distribution of Halogens in Mass Spectra')\n",
    "# plt.ylim(0, 50)  # Set y-axis limit to 100%\n",
    "# plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: f\"{x:.0f}%\"))  # Format y-axis ticks as percentages\n",
    "# plt.show()\n",
    "\n",
    "# Plot results (count)\n",
    "plt.bar(halogen_counts.keys(), halogen_counts.values())\n",
    "plt.xlabel('Halogen')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Halogens in Mass Spectra By Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing an existing MassSpecGymModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy, sys\n",
    "\n",
    "from massspecgym.data import RetrievalDataset, MassSpecDataModule\n",
    "from massspecgym.data.transforms import SpecTokenizer, MolFingerprinter\n",
    "from massspecgym.models.base import Stage\n",
    "from massspecgym.models.retrieval.base import RetrievalMassSpecGymModel\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDeepSetsRetrievalModel(RetrievalMassSpecGymModel):\n",
    "    # constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int = 128,\n",
    "        out_channels: int = 4096,  # fingerprint size\n",
    "        # out_channels: int = 4096,  # fingerprint size\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Implement your architecture.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Implement your prediction logic.\"\"\"\n",
    "        x = self.phi(x)\n",
    "        x = x.sum(dim=-2)  # sum over peaks\n",
    "        x = self.rho(x)\n",
    "        return x\n",
    "\n",
    "    def step(\n",
    "        self, batch: dict, stage: Stage\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implement your custom logic of using predictions for training and inference.\"\"\"\n",
    "        # Unpack inputs\n",
    "        x = batch[\"spec\"]  # input spectra\n",
    "        fp_true = batch[\"mol\"]  # true fingerprints\n",
    "        cands = batch[\"candidates\"]  # candidate fingerprints concatenated for a batch\n",
    "        #print(cands)\n",
    "        batch_ptr = batch[\"batch_ptr\"]  # number of candidates per sample in a batch\n",
    "        #print(batch_ptr)\n",
    "\n",
    "        # Predict fingerprint\n",
    "        fp_pred = self.forward(x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = nn.functional.mse_loss(fp_true, fp_pred)\n",
    "\n",
    "        # Calculate final similarity scores between predicted fingerprints and retrieval candidates\n",
    "        fp_pred_repeated = fp_pred.repeat_interleave(batch_ptr, dim=0)\n",
    "        scores = nn.functional.cosine_similarity(fp_pred_repeated, cands)\n",
    "\n",
    "        return dict(loss=loss, scores=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init hyperparameters\n",
    "n_peaks = 10\n",
    "fp_size = 4096\n",
    "batch_size = 2\n",
    "\n",
    "# Load dataset\n",
    "dataset = RetrievalDataset(\n",
    "    spec_transform=SpecTokenizer(n_peaks=n_peaks),\n",
    "    mol_transform=MolFingerprinter(fp_size=fp_size),\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "# Init model\n",
    "model = MyDeepSetsRetrievalModel(out_channels=fp_size)\n",
    "\n",
    "# Init trainer\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\n",
    "trainer = Trainer(accelerator=\"auto\", devices=\"auto\", max_epochs=1, logger=tb_logger)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluoride Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy, sys\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "from massspecgym.data import MassSpecDataset, MassSpecDataModule\n",
    "from massspecgym.data.transforms import SpecTokenizer, MolFingerprinter\n",
    "from massspecgym.models.base import Stage\n",
    "from massspecgym.models.retrieval.base import MassSpecGymModel\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from massspecgym.models.base import Stage\n",
    "from dreams.api import PreTrainedModel\n",
    "from dreams.models.dreams.dreams import DreaMS as DreaMSModel\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n",
      "[0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from massspecgym.data.transforms import MolToHalogensVector\n",
    "\n",
    "\n",
    "# Example usage\n",
    "checker = MolToHalogensVector()\n",
    "smiles_string = \"CC(F)(F)F\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)\n",
    "# Example usage\n",
    "smiles_string = \"CCBr\"\n",
    "halogen_vector = checker.from_smiles(smiles_string)\n",
    "print(halogen_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    mgf_pth = Path(\"/teamspace/studios/this_studio/MassSpecGym/data/debug/example_5_spectra.mgf\")\n",
    "    split_pth = Path(\"/teamspace/studios/this_studio/MassSpecGym/data/debug/example_5_spectra_split.tsv\")\n",
    "else:\n",
    "    mgf_pth = None\n",
    "    split_pth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "class FluorineBalancedDataset(MassSpecDataset):\n",
    "    \"\"\"\n",
    "    Dataset containing balanced set of Fluorine training examples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs,):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def load_data(self):\n",
    "        super().load_data()\n",
    "        checker = MolToHalogensVector()\n",
    "        num_negatives = 0\n",
    "        num_positives = 0\n",
    "        indices_to_drop = []\n",
    "        for idx, row in self.metadata.iterrows():\n",
    "            halogen_vector = checker.from_smiles(row.get(\"smiles\"))\n",
    "            if halogen_vector[0] == 0 and row.get(\"fold\") == 'train': \n",
    "                if num_negatives >= 8718:\n",
    "                    indices_to_drop.append(idx)\n",
    "                else:\n",
    "                    num_negatives += 1\n",
    "\n",
    "        self.metadata = self.metadata.drop(indices_to_drop).reset_index(drop=True)\n",
    "        self.spectra = self.spectra.drop(indices_to_drop).reset_index(drop=True)\n",
    "        print(\"---train\", len(self.metadata[self.metadata['fold'] == 'train']))\n",
    "        print(\"---val\", len(self.metadata[self.metadata['fold'] == 'val']))\n",
    "\n",
    "\n",
    "# base model contains definitions for step and on_batch_end\n",
    "class HalogenPredMassSpecGymModel(MassSpecGymModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float=0.8,\n",
    "        gamma: float=0.5,\n",
    "        batch_size: int=64,\n",
    "        threshold: float=0.5,\n",
    "        pos_weight: float=1.0,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = torch.tensor([1-alpha, alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.threshold = threshold\n",
    "        self.pos_weight = pos_weight\n",
    "        # training metrics\n",
    "        self.train_num_actual_positives = 0\n",
    "        self.train_num_predicted_positives = 0\n",
    "        self.train_num_true_positives = 0\n",
    "        # validation metrics\n",
    "        self.val_num_actual_positives = 0\n",
    "        self.val_num_predicted_positives = 0\n",
    "        self.val_num_true_positives = 0\n",
    "\n",
    "    # def step(\n",
    "    #     self, batch: dict, stage: Stage\n",
    "    # ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    #     return { 'loss': torch.tensor(0.0, requires_grad=True) } \n",
    " \n",
    "    def step(\n",
    "        self, batch: dict, stage: Stage\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implement your custom logic of using predictions for training and inference.\"\"\"\n",
    "        # Unpack inputs\n",
    "        x = batch[\"spec\"]  \n",
    "        # input spectra [batch_size, num_peaks + 1, 2]\n",
    "\n",
    "        halogen_vector_true = batch[\"mol\"]\n",
    "        # Extract the 1st column --> fluorine predictions\n",
    "        true_values = halogen_vector_true[:, 0] # shape [batch_size]\n",
    "        predicted_probs = self.forward(x) # shape [batch_size x 1]\n",
    "        \n",
    "        if DEBUG:\n",
    "            predicted_probs = predicted_probs[0] # for testing\n",
    "        else:\n",
    "            predicted_probs = predicted_probs.squeeze() # shape [batch_size]\n",
    "\n",
    "        # BCE Loss & apply weight to positive examples\n",
    "        #weight = torch.where(true_values == 1.0, self.pos_weight, 1)\n",
    "        #bce_loss = nn.BCELoss(weight=weight)\n",
    "        #loss = bce_loss(predicted_probs, true_values)\n",
    "        \n",
    "        # if DEBUG and stage.to_pref() == 'val_':\n",
    "        #     print('\\n--true_values', true_values)\n",
    "        #     print('\\n--predicted_probs', predicted_probs)\n",
    "        \n",
    "        # return { 'loss': loss } \n",
    "\n",
    "        # Focal Loss: https://amaarora.github.io/posts/2020-06-29-FocalLoss.html # \n",
    "        # Increase loss for minority misclassification (F = 1 but predicted as 0) and \n",
    "        # decreases loss for majority class misclassification (F = 0 but predicted as 1)\n",
    "        # Our MassEpcGym training data is skewed with only 5% of molecules containing Fluorine\n",
    "        bce_loss = nn.BCELoss(reduction='none')\n",
    "        loss = bce_loss(predicted_probs, true_values)\n",
    "        targets = true_values.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-loss)\n",
    "        F_loss = at * (1 - pt)**self.gamma * loss\n",
    "        return { 'loss': F_loss.mean() } \n",
    "\n",
    "\n",
    "    def on_batch_end(\n",
    "        self, outputs: [], batch: dict, batch_idx: int, stage: Stage\n",
    "    ) -> None:\n",
    "        x = batch[\"spec\"]\n",
    "        halogen_vector_true = batch[\"mol\"] # shape [batch_size]\n",
    "        halogen_vector_pred = self.forward(x) # shape [batch_size x 1]\n",
    "        halogen_vector_pred_binary = torch.where(halogen_vector_pred >= self.threshold, 1, 0)\n",
    "\n",
    "        # Extract the 1st column --> fluorine predictions\n",
    "        true_values = halogen_vector_true[:, 0].cpu().numpy() # shape [batch_size]\n",
    "        pred_values = halogen_vector_pred_binary.squeeze().cpu().numpy() # shape [batch_size]\n",
    "\n",
    "        if stage.to_pref() == 'train_':\n",
    "            self.train_num_actual_positives += np.sum(true_values) # all true positives\n",
    "            self.train_num_predicted_positives += np.sum(pred_values) # all predicted positives\n",
    "            true_positives = np.logical_and(true_values, pred_values)\n",
    "            self.train_num_true_positives += np.sum(true_positives)\n",
    "        elif stage.to_pref() == 'val_':\n",
    "            self.val_num_actual_positives += np.sum(true_values) # all true positives\n",
    "            self.val_num_predicted_positives += np.sum(pred_values) # all predicted positives\n",
    "            true_positives = np.logical_and(true_values, pred_values)\n",
    "            self.val_num_true_positives += np.sum(true_positives)\n",
    "\n",
    "        self.log_dict({ f\"{stage.to_pref()}/loss\": outputs['loss'] },\n",
    "                prog_bar=True,\n",
    "                on_epoch=True,\n",
    "                batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def _reset_metrics_train(self):\n",
    "        self.train_num_actual_positives = 0\n",
    "        self.train_num_predicted_positives = 0\n",
    "        self.train_num_true_positives = 0\n",
    "\n",
    "    def _reset_metrics_val(self):\n",
    "        self.val_num_actual_positives = 0\n",
    "        self.val_num_predicted_positives = 0\n",
    "        self.val_num_true_positives = 0\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        self._reset_metrics_train()\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self._reset_metrics_val()\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        precision = self.train_num_true_positives/self.train_num_predicted_positives if self.train_num_predicted_positives != 0 else 0\n",
    "        recall = self.train_num_true_positives/self.train_num_actual_positives if self.train_num_actual_positives != 0 else 0\n",
    "        f1_score = (2*precision*recall)/(precision + recall) if (precision + recall) != 0 else 0\n",
    "        self.log_dict({   \n",
    "                f\"train_/num_actual_positives\" : self.train_num_actual_positives, \n",
    "                f\"train_/num_predicted_positives\": self.train_num_predicted_positives, \n",
    "                f\"train_/num_true_positives\": self.train_num_true_positives,\n",
    "                f\"train_/precision\": precision,\n",
    "                f\"train_/recall\": recall,\n",
    "                f\"train_/f1_score\": f1_score\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False\n",
    "        )\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        precision = self.val_num_true_positives/self.val_num_predicted_positives if self.val_num_predicted_positives != 0 else 0\n",
    "        recall = self.val_num_true_positives/self.val_num_actual_positives if self.val_num_actual_positives != 0 else 0\n",
    "        f1_score = (2*precision*recall)/(precision + recall) if (precision + recall) != 0 else 0\n",
    "        self.log_dict({   \n",
    "                f\"val_/num_actual_positives\" : self.val_num_actual_positives, \n",
    "                f\"val_/num_predicted_positives\": self.val_num_predicted_positives, \n",
    "                f\"val_/num_true_positives\": self.val_num_true_positives,\n",
    "                f\"val_/precision\": precision,\n",
    "                f\"val_/recall\": recall,\n",
    "                f\"val_/f1_score\": f1_score\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False\n",
    "        )\n",
    "\n",
    "# final model containing the network definition\n",
    "class HalogenDetectorDreams(HalogenPredMassSpecGymModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.spec_encoder = PreTrainedModel.from_ckpt(\n",
    "            # ckpt_path should be replaced with the path to the ssl_model.ckpt model downloaded from https://zenodo.org/records/10997887\n",
    "            ckpt_path=\"https://zenodo.org/records/10997887/files/ssl_model.ckpt?download=1\", ckpt_cls=DreaMSModel, n_highest_peaks=60\n",
    "        ).model.train()\n",
    "        #self.lin_out = nn.Linear(1024, 4) # for the 4 halogens (F, Cl, Br, I)\n",
    "        self.lin_out = nn.Linear(1024, 1) # for F\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spec_encoder(x)[:, 0, :] # to get the precursor peak token embedding \n",
    "        x = F.sigmoid(self.lin_out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Init hyperparameters\n",
    "n_peaks = 60\n",
    "threshold = 0.50\n",
    "pos_weight = 10\n",
    "gammas = [0.5]\n",
    "\n",
    "if DEBUG:\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 64\n",
    "\n",
    "for gamma in gammas:\n",
    "    # Load dataset\n",
    "    dataset = FluorineBalancedDataset(\n",
    "        spec_transform=SpecTokenizer(n_peaks=n_peaks),\n",
    "        mol_transform = MolToHalogensVector(),\n",
    "        pth=mgf_pth,\n",
    "    )\n",
    "\n",
    "    # Init data module\n",
    "    data_module = MassSpecDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        split_pth=split_pth,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Init model\n",
    "    model = HalogenDetectorDreams(\n",
    "        threshold=threshold,\n",
    "        pos_weight=pos_weight,\n",
    "        alpha=0.8,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    \n",
    "    # initialise the wandb logger and name your wandb project\n",
    "    wandb_logger = WandbLogger(project='MassSpecGym-DreaMS-HalogenDetection-FocalLoss')\n",
    "\n",
    "    # add your batch size to the wandb config\n",
    "    wandb_logger.experiment.config[\"batch_size\"] = batch_size\n",
    "    wandb_logger.experiment.config[\"n_peaks\"] = n_peaks\n",
    "    wandb_logger.experiment.config[\"threshold\"] = threshold\n",
    "    wandb_logger.experiment.config[\"pos_weight\"] = pos_weight\n",
    "    wandb_logger.experiment.config[\"alpha\"] = model.alpha\n",
    "    wandb_logger.experiment.config[\"gamma\"] = model.gamma\n",
    "\n",
    "    # Init trainer\n",
    "    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", max_epochs=5, logger=wandb_logger, val_check_interval=0.01)\n",
    "\n",
    "    # Validate before training\n",
    "    data_module.prepare_data()  # Explicit call needed for validate before fit\n",
    "    data_module.setup()  # Explicit call needed for validate before fit\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "    # # Train\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # [optional] finish the wandb run, necessary in notebooks\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the network\n",
    "hidden_channels = 5\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, hidden_channels),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_channels, hidden_channels),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# Initialize the network\n",
    "net = net.float()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.ones(5, 10, 2)\n",
    "# Forward pass\n",
    "output = net(input_tensor)\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = np.array([1, 1., 1])\n",
    "pred_values = np.array([0.0, 1, 1])\n",
    "\n",
    "precision_score(true_values, pred_values)\n",
    "recall_score(true_values, pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from massspecgym.models.base import Stage\n",
    "from dreams.api import PreTrainedModel\n",
    "from dreams.models.dreams.dreams import DreaMS as DreaMSModel\n",
    "\n",
    "# Example forward pass (not needed to explicitly initialize the DataLoader if you are using MassSpecGym)\n",
    "from massspecgym.data.datasets import MassSpecDataset\n",
    "from massspecgym.data.transforms import SpecTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MassSpecDataset(\n",
    "    spec_transform=SpecTokenizer(n_peaks=n_peaks),\n",
    "    mol_transform = MolToHalogensVector()\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "model = HalogenDetectorDreams()\n",
    "\n",
    "dummy_batch = next(iter(dataloader))\n",
    "dummy_output = model(dummy_batch)\n",
    "print(dummy_output)  # Should print a tensor of shape (4, 4) containing halogen probabilties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "input = torch.tensor([[2.0, 3.0, 5.0], [2.0, 3.0, 5.0]])\n",
    "print(m(input))\n",
    "target = torch.tensor([[1.0, 0.0, 1.0], [1.0, 0.0, 0.0]])\n",
    "print(target)\n",
    "output = loss(m(input), target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a1 = np.array([1, 1, 1])\n",
    "a2 = np.array([0, 1, 1])\n",
    "\n",
    "print(np.sum(np.logical_and(a1, a2))) # should return 2\n",
    "\n",
    "a1 = np.array([1, 1, 0])\n",
    "a2 = np.array([0, 0, 1])\n",
    "\n",
    "print(np.sum(np.logical_and(a1, a2))) # should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch\n",
    "    alpha = torch.tensor([0.7, 0.3]).cuda()\n",
    "    true_values = torch.tensor([1, 0, 1, 1]).cuda()\n",
    "    targets = true_values.type(torch.long)\n",
    "    targets.data.view(-1)\n",
    "    at = alpha.gather(0, targets.data.view(-1))\n",
    "    at"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
