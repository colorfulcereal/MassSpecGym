{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import massspecgym.utils as utils\n",
    "from massspecgym.data import MassSpecDataset, RetrievalDataset, MassSpecDataModule\n",
    "from massspecgym.data.transforms import SpecTokenizer, MolFingerprinter, SpecBinner\n",
    "from massspecgym.models.retrieval import DeepSetsRetrieval, RandomRetrieval, FingerprintFFNRetrieval, FromDictRetrieval\n",
    "from massspecgym.models.de_novo import DummyDeNovo, RandomDeNovo, SmilesTransformer\n",
    "from massspecgym.models.tokenizers import SmilesBPETokenizer, SelfiesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    mgf_pth = Path(\"../data/debug/example_5_spectra.mgf\")\n",
    "    candidates_pth = Path(\"../data/debug/example_5_spectra_candidates.json\")\n",
    "    split_pth=Path(\"../data/debug/example_5_spectra_split.tsv\")\n",
    "else:\n",
    "    # Use default benchmark paths\n",
    "    # mgf_pth = None\n",
    "    # candidates_pth = None\n",
    "    # split_pth = None\n",
    "    mgf_pth = Path(\"../data/MassSpecGym_with_test/MassSpecGym_with_test.tsv\")\n",
    "    candidates_pth = Path(\"../data/MassSpecGym_with_test/MassSpecGym_retrieval_candidates_formula_with_test.json\")\n",
    "    split_pth = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets model on the fingerprint retrieval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RetrievalDataset(\n",
    "    pth=mgf_pth,\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform=MolFingerprinter(),\n",
    "    candidates_pth=candidates_pth,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# Uncomment the paths to use debugging data containing only 5 spectra\n",
    "dataset = RetrievalDataset(\n",
    "    pth=mgf_pth,\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform=MolFingerprinter(),\n",
    "    candidates_pth=candidates_pth,\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=3\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = DeepSetsRetrieval(\n",
    "    bootstrap_metrics=True,\n",
    "    df_test_path='./df_test.pkl',\n",
    "    out_channels=2048,\n",
    "    fourier_features=True\n",
    ")\n",
    "# model = RandomRetrieval()\n",
    "\n",
    "# Init logger\n",
    "# You may need to run wandb init first to use the wandb logger\n",
    "# Alternatively set logger = None in Trainer below not to use wandb\n",
    "project = \"MassSpecGymRetrieval\"\n",
    "name = \"DeepSets\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=50, logger=logger, log_every_n_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: adamoyoung. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250203_234710-5qzqvgid</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamoyoung/MassSpecGymRetrieval/runs/5qzqvgid' target=\"_blank\">DeepSets</a></strong> to <a href='https://wandb.ai/adamoyoung/MassSpecGymRetrieval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamoyoung/MassSpecGymRetrieval' target=\"_blank\">https://wandb.ai/adamoyoung/MassSpecGymRetrieval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamoyoung/MassSpecGymRetrieval/runs/5qzqvgid' target=\"_blank\">https://wandb.ai/adamoyoung/MassSpecGymRetrieval/runs/5qzqvgid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f793fab4d841eb88c34bd99e86c45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      " val_fingerprint_cos_sim    0.1643836945295334\n",
      "     val_hit_rate@1                 0.0\n",
      "     val_hit_rate@20                1.0\n",
      "     val_hit_rate@5                 1.0\n",
      "        val_loss            0.8356163501739502\n",
      "       val_mces@1                  19.5\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "\n",
      "   | Name                    | Type             | Params\n",
      "--------------------------------------------------------------\n",
      "0  | ff                      | FourierFeatures  | 6.0 K \n",
      "1  | ff_proj_mz              | Linear           | 4.9 M \n",
      "2  | ff_proj_i               | Linear           | 206   \n",
      "3  | phi                     | MLP              | 525 K \n",
      "4  | rho                     | MLP              | 1.3 M \n",
      "5  | loss_fn                 | CosSimLoss       | 0     \n",
      "6  | val_fingerprint_cos_sim | CosineSimilarity | 0     \n",
      "7  | val_hit_rate@1          | MeanMetric       | 0     \n",
      "8  | val_hit_rate@5          | MeanMetric       | 0     \n",
      "9  | val_hit_rate@20         | MeanMetric       | 0     \n",
      "10 | val_mces@1              | MeanMetric       | 0     \n",
      "--------------------------------------------------------------\n",
      "6.7 M     Trainable params\n",
      "6.0 K     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.003    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d501047564d4a3da5a5d1824ea68b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e803a7384b34567b7a079ede0aec44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b3628b252740a58566ec5916ceb481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f469cab0c4d432681b0dd2314638818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183b1f5c64a14bd09dcb05387b3169bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bb18e179544b63be262fc92d171323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cf4ad487794f69af4aa6a5e0d97e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6a37ef6bf04553a3d5829652081748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c8cfb7263a4c23b6c75f0469d704f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413fbe30acc648a0b72ba1115c436a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4851d77006a4caf9159959437ccfef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87061a21e3084ca2a113bbb06d186e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18b208503a0450e895d0fb2c486c993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d9e79f84344541b93b83c36068966b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa6dd066094c01bd077e0bc16beecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151d84de949847629dee908fbd069df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19559d3d78d44abf851741ffcea3476c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9180fb5074b94c868f8e1eb70002f48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733b0b96671d4256b8d86f0067915e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d330d9901e84ea1a66258f0c5712b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04cb81629eb4e638b355ff1be22940f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca0f02b93d648dbb362c88ac11ea6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bee0c4fd3e34ac49ad21248b9934893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a138a684db84a37ad701381c4940cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb9b88b7465492fa93ef993a16b5dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e73250c6184eea98b1be2b016cfa9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e341d5da2b1547439ac6b35b8cd254eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f810da99fb3e44e4bf86a8ae3b76383c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bed84e0b65b4182a04f412f9364b3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9092f2eb637a4658a86f72ff9f54e62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928a69b89fd49fcbd403ab478682ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f958a43c3125448c83e982e2e08f1a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf139061c47484887487f16e0e59684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f879679b704abcb397eb87f772bf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e87261e300a49ceb0ec106164df765c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b771acb757664f0ba0cb1d2a62322c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ccf71181f540c18af030a4e3ed53e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd45bb5d0f6243be890571a94e06d394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81019305f3d54022908567a750500062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0422aefda3f24336b44604c74b01883f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c756c08da86342f3992deb902ec5c9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30002b6b86a84814aae0747909e4c9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d22fe48e354bdab745c8542e100630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b291faa99704476bc6e6008cfc1d8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c958c9678e924de39fd0fb0a02c69d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8c4bca9e134e6cb60d22210e47142f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e145a420650844c5bec3eb4cc4708a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e768101a01945d1b75386aa8310469c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa67da782cc4aae81d87f0f689430fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc19470deead4d70bebb2f6eb3fb19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fb9874f6be4dbd815a8c50ac8a8674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7b62a6b05143b796581b0e980c5e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12c246c446444ef8fdf29e2edafac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "test_fingerprint_cos_sim    0.22373272478580475\n",
      "     test_hit_rate@1                0.0\n",
      "   test_hit_rate@1_std              nan\n",
      "    test_hit_rate@20                0.0\n",
      "  test_hit_rate@20_std              nan\n",
      "     test_hit_rate@5                0.0\n",
      "   test_hit_rate@5_std              nan\n",
      "        test_loss           0.7762672901153564\n",
      "       test_mces@1                 28.0\n",
      "     test_mces@1_std                nan\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_fingerprint_cos_sim': 0.22373272478580475,\n",
       "  'test_loss': 0.7762672901153564,\n",
       "  'test_hit_rate@1': 0.0,\n",
       "  'test_hit_rate@1_std': nan,\n",
       "  'test_hit_rate@5': 0.0,\n",
       "  'test_hit_rate@5_std': nan,\n",
       "  'test_hit_rate@20': 0.0,\n",
       "  'test_hit_rate@20_std': nan,\n",
       "  'test_mces@1': 28.0,\n",
       "  'test_mces@1_std': nan}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate before training\n",
    "data_module.prepare_data()  # Explicit call needed for validate before fit\n",
    "data_module.setup()  # Explicit call needed for validate before fit\n",
    "trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "# Train\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random baseline on the fingerprint retrieval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "fp_size = 4096\n",
    "\n",
    "# Load dataset\n",
    "dataset = RetrievalDataset(\n",
    "    pth=mgf_pth,\n",
    "    spec_transform=SpecBinner(),\n",
    "    mol_transform=MolFingerprinter(fp_size=fp_size),\n",
    "    candidates_pth=candidates_pth,\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = RandomRetrieval()\n",
    "\n",
    "# Init logger\n",
    "# You may need to run wandb init first to use the wandb logger\n",
    "# Alternatively set logger = None in Trainer below not to use wandb\n",
    "project = \"MassSpecGymRetrieval\"\n",
    "name = \"RandomFFN\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=50, logger=logger, log_every_n_steps=50\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerpint FFN model on the fingerprint retrieval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "fp_size = 4096\n",
    "\n",
    "# Load dataset\n",
    "dataset = RetrievalDataset(\n",
    "    pth=mgf_pth,\n",
    "    spec_transform=SpecBinner(),\n",
    "    mol_transform=MolFingerprinter(fp_size=fp_size),\n",
    "    candidates_pth=candidates_pth,\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = FingerprintFFNRetrieval(\n",
    "    in_channels=1005,\n",
    "    out_channels=fp_size,\n",
    ")\n",
    "\n",
    "# Init logger\n",
    "# You may need to run wandb init first to use the wandb logger\n",
    "# Alternatively set logger = None in Trainer below not to use wandb\n",
    "project = \"MassSpecGymRetrieval\"\n",
    "name = \"FingerprintFFN_debug\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=50, logger=logger, log_every_n_steps=50\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIST on the fingerprint retrieval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp_size = 4096\n",
    "\n",
    "# # Load dataset\n",
    "# dataset = RetrievalDataset(\n",
    "#     pth=mgf_pth,\n",
    "#     spec_transform=SpecBinner(),\n",
    "#     mol_transform=MolFingerprinter(fp_size=fp_size),\n",
    "#     candidates_pth=candidates_pth,\n",
    "# )\n",
    "\n",
    "# # Init data module\n",
    "# data_module = MassSpecDataModule(\n",
    "#     dataset=dataset,\n",
    "#     split_pth=split_pth,\n",
    "#     batch_size=64\n",
    "# )\n",
    "\n",
    "# # Init model\n",
    "# df = pd.read_pickle('fp_preds_MassSpecGym_df.pkl')\n",
    "# dct = dict(zip(df['name'], df['fp_predict']))\n",
    "# model = FromDictRetrieval(dct=dct)\n",
    "\n",
    "# # Init logger\n",
    "# # You may need to run wandb init first to use the wandb logger\n",
    "# # Alternatively set logger = None in Trainer below not to use wandb\n",
    "# project = \"MassSpecGymRetrieval\"\n",
    "# name = \"MIST\"\n",
    "# logger = pl.loggers.WandbLogger(\n",
    "#     project=project,\n",
    "#     name=name,\n",
    "#     tags=[],\n",
    "#     log_model=False,\n",
    "# )\n",
    "\n",
    "# # Init trainer\n",
    "# trainer = Trainer(\n",
    "#     accelerator=\"cpu\", max_epochs=50, logger=logger, log_every_n_steps=50\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy model on the de novo generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# Uncomment the paths to use debugging data containing only 5 spectra\n",
    "dataset = MassSpecDataset(\n",
    "    pth=mgf_pth,\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform=None\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = DummyDeNovo(\n",
    "    df_test_path='./df_test.pkl'\n",
    ")\n",
    "\n",
    "# Init logger\n",
    "# You may need to run wandb init first to use the wandb logger\n",
    "# Alternatively set logger = None in Trainer below not to use wandb\n",
    "project = \"MassSpecGymDeNovo\"\n",
    "name = \"RandomBasline\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=50, logger=logger, log_every_n_steps=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De novo SMILES transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on 3947573 SMILES strings.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3278bfe5ed13439eb64c23441de633f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      Validate metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_loss               9.059257507324219\n",
      "    val_num_valid_mols                 1.0\n",
      "    val_top_10_accuracy                0.0\n",
      "val_top_10_max_tanimoto_sim   0.036036036908626556\n",
      "   val_top_10_mces_dist               100.0\n",
      "    val_top_1_accuracy                 0.0\n",
      "val_top_1_max_tanimoto_sim    0.036036036908626556\n",
      "    val_top_1_mces_dist               100.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "\n",
      "   | Name                        | Type             | Params\n",
      "------------------------------------------------------------------\n",
      "0  | src_encoder                 | Linear           | 1.5 K \n",
      "1  | tgt_embedding               | Embedding        | 2.7 M \n",
      "2  | transformer                 | Transformer      | 29.4 M\n",
      "3  | tgt_decoder                 | Linear           | 2.7 M \n",
      "4  | criterion                   | CrossEntropyLoss | 0     \n",
      "5  | val_num_valid_mols          | MeanMetric       | 0     \n",
      "6  | val_top_1_mces_dist         | MeanMetric       | 0     \n",
      "7  | val_top_1_max_tanimoto_sim  | MeanMetric       | 0     \n",
      "8  | val_top_1_accuracy          | MeanMetric       | 0     \n",
      "9  | val_top_10_mces_dist        | MeanMetric       | 0     \n",
      "10 | val_top_10_max_tanimoto_sim | MeanMetric       | 0     \n",
      "11 | val_top_10_accuracy         | MeanMetric       | 0     \n",
      "------------------------------------------------------------------\n",
      "34.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "34.8 M    Total params\n",
      "139.053   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e849c58ca74b4caf5cac38a13c3c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdab4fc882de41e78ffb2ddc28d7f0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:44] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:44] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:44] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:44] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:44] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:44] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:50] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:50] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:50] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:50] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:50] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:50] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:56] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:56] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:56] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:56] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:56] Invalid InChI prefix in generating InChI Key\n",
      "[23:50:56] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:02] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:02] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:02] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:02] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:02] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:02] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:08] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:08] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:08] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:08] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:08] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:08] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:14] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:14] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:14] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:14] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:14] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:14] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:21] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:21] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:21] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:21] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:21] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:21] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:26] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:32] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:38] Invalid InChI prefix in generating InChI Key\n",
      "[23:51:44] non-ring atom 0 marked aromatic\n",
      "[23:51:44] non-ring atom 0 marked aromatic\n",
      "[23:51:44] non-ring atom 0 marked aromatic\n",
      "[23:51:51] non-ring atom 0 marked aromatic\n",
      "[23:51:51] non-ring atom 0 marked aromatic\n",
      "[23:51:51] non-ring atom 0 marked aromatic\n",
      "[23:51:57] non-ring atom 0 marked aromatic\n",
      "[23:51:57] non-ring atom 0 marked aromatic\n",
      "[23:51:57] non-ring atom 0 marked aromatic\n",
      "[23:52:03] non-ring atom 0 marked aromatic\n",
      "[23:52:03] non-ring atom 0 marked aromatic\n",
      "[23:52:03] non-ring atom 0 marked aromatic\n",
      "[23:52:09] non-ring atom 0 marked aromatic\n",
      "[23:52:09] non-ring atom 0 marked aromatic\n",
      "[23:52:09] non-ring atom 0 marked aromatic\n",
      "[23:52:15] non-ring atom 0 marked aromatic\n",
      "[23:52:15] non-ring atom 0 marked aromatic\n",
      "[23:52:15] non-ring atom 0 marked aromatic\n",
      "[23:52:21] non-ring atom 0 marked aromatic\n",
      "[23:52:21] non-ring atom 0 marked aromatic\n",
      "[23:52:21] non-ring atom 0 marked aromatic\n",
      "[23:52:27] non-ring atom 0 marked aromatic\n",
      "[23:52:27] non-ring atom 0 marked aromatic\n",
      "[23:52:27] non-ring atom 0 marked aromatic\n",
      "[23:52:33] non-ring atom 0 marked aromatic\n",
      "[23:52:33] non-ring atom 0 marked aromatic\n",
      "[23:52:33] non-ring atom 0 marked aromatic\n",
      "[23:52:39] non-ring atom 0 marked aromatic\n",
      "[23:52:39] non-ring atom 0 marked aromatic\n",
      "[23:52:39] non-ring atom 0 marked aromatic\n",
      "[23:52:45] non-ring atom 0 marked aromatic\n",
      "[23:52:45] non-ring atom 0 marked aromatic\n",
      "[23:52:45] non-ring atom 0 marked aromatic\n",
      "[23:52:51] non-ring atom 0 marked aromatic\n",
      "[23:52:51] non-ring atom 0 marked aromatic\n",
      "[23:52:51] non-ring atom 0 marked aromatic\n",
      "[23:52:57] non-ring atom 0 marked aromatic\n",
      "[23:52:57] non-ring atom 0 marked aromatic\n",
      "[23:52:57] non-ring atom 0 marked aromatic\n",
      "[23:53:03] non-ring atom 0 marked aromatic\n",
      "[23:54:48] SMILES Parse Error: syntax error while parsing: CC/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccc\n",
      "[23:54:48] SMILES Parse Error: Failed parsing SMILES 'CC/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccc' for input: 'CC/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccc'\n",
      "[23:54:48] SMILES Parse Error: syntax error while parsing: C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccccccccccccccccccccccccccc\n",
      "[23:54:48] SMILES Parse Error: Failed parsing SMILES 'C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccccccccccccccccccccccccccc' for input: 'C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:54:53] SMILES Parse Error: syntax error while parsing: C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccccccccccccccccccccccccccc\n",
      "[23:54:53] SMILES Parse Error: Failed parsing SMILES 'C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccccccccccccccccccccccccccc' for input: 'C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c(C@H]2c(C@H]2c(C)cccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:54:53] SMILES Parse Error: syntax error while parsing: C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c2c(C@H]2c(C@H]2c(C@H]2c(C@H]2c2c(C)cccccccccccccccccccccccc\n",
      "[23:54:53] SMILES Parse Error: Failed parsing SMILES 'C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c2c(C@H]2c(C@H]2c(C@H]2c(C@H]2c2c(C)cccccccccccccccccccccccc' for input: 'C/C(=O[C@H]2c(C@H]2c(C@H]2c(C@@H]2c(C@H]2c2c(C@H]2c(C@H]2c(C@H]2c(C@H]2c2c(C)cccccccccccccccccccccccc'\n",
      "[23:54:53] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2[C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c\n",
      "[23:54:53] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2[C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c' for input: 'C/C1=C/CC[C@H]2(C@@H]2[C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c'\n",
      "[23:54:59] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c\n",
      "[23:54:59] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c'\n",
      "[23:54:59] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c(C)c(C)ccc\n",
      "[23:54:59] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c(C)c(C)ccc' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c2c(C)c(C)ccc'\n",
      "[23:54:59] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c\n",
      "[23:54:59] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c'\n",
      "[23:55:05] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:05] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:05] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c\n",
      "[23:55:05] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c'\n",
      "[23:55:05] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(\n",
      "[23:55:05] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2('\n",
      "[23:55:11] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c\n",
      "[23:55:11] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c'\n",
      "[23:55:11] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:11] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:11] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c\n",
      "[23:55:11] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2c2c2c2c2c2c2c2c2c2c2c2c2c'\n",
      "[23:55:17] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:17] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:17] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:17] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:17] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(\n",
      "[23:55:17] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(' for input: 'C/C1=C/CC[C@H]2(C@@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2(C@H]2('\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699d893f7f9b4f4a94fc0f307ba13d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:55:20] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:20] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:26] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:26] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:26] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:26] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:26] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:26] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:32] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@\n",
      "[23:55:32] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@'\n",
      "[23:55:32] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2\n",
      "[23:55:32] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2'\n",
      "[23:55:32] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2\n",
      "[23:55:32] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2'\n",
      "[23:55:38] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2\n",
      "[23:55:38] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2'\n",
      "[23:55:38] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2\n",
      "[23:55:38] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2'\n",
      "[23:55:38] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2\n",
      "[23:55:38] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2'\n",
      "[23:55:44] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1\n",
      "[23:55:44] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1'\n",
      "[23:55:44] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:44] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:44] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1\n",
      "[23:55:44] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1'\n",
      "[23:55:47] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1\n",
      "[23:55:47] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@@H]2CC1'\n",
      "[23:55:47] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:47] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:47] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:47] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:49] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:49] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:49] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:49] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:49] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:49] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:55] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:55] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:55] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:55] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:55] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:55] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:58] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:58] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:58] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:58] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:55:58] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:55:58] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:03] SMILES Parse Error: unclosed ring for input: 'COc1nccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:56:03] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:03] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:03] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:03] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:09] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:09] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:09] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:09] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:09] SMILES Parse Error: unclosed ring for input: 'COc1nccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:56:15] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:15] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:15] SMILES Parse Error: unclosed ring for input: 'COc1nccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:56:15] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:15] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:21] SMILES Parse Error: unclosed ring for input: 'COc1nccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:56:21] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:21] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:21] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:21] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:28] SMILES Parse Error: unclosed ring for input: 'COc1nccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:56:28] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:28] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:28] SMILES Parse Error: syntax error while parsing: C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1\n",
      "[23:56:28] SMILES Parse Error: Failed parsing SMILES 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1' for input: 'C/C1=C/CC[C)O[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:33] SMILES Parse Error: extra open parentheses for input: 'COc1nccccccccccccccccccccccccccccc(C(=O)ccccccccccccccccccccccccccccccccccccccccccccccccccccccccc(Cl)c'\n",
      "[23:56:33] SMILES Parse Error: ring closure 2 duplicates bond between atom 9 and atom 10 for input: 'C/C1=C/CC[C@@]2(C)O[C@@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:33] SMILES Parse Error: ring closure 2 duplicates bond between atom 9 and atom 10 for input: 'C/C1=C/CC[C@@]2(C)O[C@@H]2[C@H]2[C@H]2OC(=O)[C@H]2CC1'\n",
      "[23:56:39] SMILES Parse Error: syntax error while parsing: COc1nccccccccccccccccccccc(C(=O)ccccccccccccccccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)cccccccccccc(\n",
      "[23:56:39] SMILES Parse Error: Failed parsing SMILES 'COc1nccccccccccccccccccccc(C(=O)ccccccccccccccccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)cccccccccccc(' for input: 'COc1nccccccccccccccccccccc(C(=O)ccccccccccccccccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)cccccccccccc('\n",
      "[23:56:45] SMILES Parse Error: extra open parentheses for input: 'COc1nccccccccccccccccccccc(C(=O)cccccccccccccccccc(Cl)cccccccccccc(Cl)cccccccccccc(Cl)cccccccccccc(Cl)ccc'\n",
      "[23:56:51] SMILES Parse Error: extra open parentheses for input: 'COc1nccccccccccccccccccc(C(=O)cccccccccccccc(Cl)ccccccccccc(Cl)ccccccccccc(Cl)ccccccccccc(Cl)ccccccccc(Cl)'\n",
      "[23:56:57] SMILES Parse Error: extra open parentheses for input: 'COc1ncccccccccccccccc(C(=O)ccccccccc(Cl)ccccccccc(Cl)ccccccccc(Cl)ccccccccc(Cl)cccccccc(Cl)cccccccc(Cl)cccc'\n",
      "[23:57:03] SMILES Parse Error: extra open parentheses for input: 'COc1nccccccccccccccc(C(=O)cccccc(Cl)ccccccccc(Cl)ccccccccc(Cl)cccccccc(Cl)ccccccc(Cl)cccccccc(Cl)ccccccc(Cl)'\n",
      "[23:57:09] SMILES Parse Error: extra open parentheses for input: 'COc1nccccccccccccc(C(=O)Ncccccc(Cl)cccccccccc(Cl)ccccccccccc(Cl)cccccccccc(Cl)ccccccccc(Cl)ccccccccc(Cl)ccc'\n",
      "[23:57:15] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(c(c(c(c(c(C4)CC3)c(F)cccccccccccccccccccccccccc(F)cccccccccccc(F)c'\n",
      "[23:57:21] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(c(c(c(c(c(C4)CC3)c(F)ccccccccccccccccccccccccccc(F)cccccccccccc(F)'\n",
      "[23:57:27] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(c(c(c(c(c(C4)CC3)c(F)cccccccccccccccccccccccccc(F)ccccccccccc(F)cc'\n",
      "[23:57:33] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(c(c(c(c(C4)CC3)c(F)ccccccccccccccccccc(F)cccccccccc(F)cccccccccc(F'\n",
      "[23:57:39] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(c(c(c(c(c(C4)CC3)c(F)cccccccccccccccccccccc(F)cccccccccc(F)ccccccc'\n",
      "[23:57:45] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(c(c(N3CCN(C4)cccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:57:51] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(N3CCN(C4)cccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:57:56] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(c(N3CCN(C4)cccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:02] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(N3CCN(C4)cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:08] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc(c(c(c(c(c(c(c(c(c(N3CCN(C4)cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:14] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:20] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:26] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccccccccccccccc(F)ccccccccccccccccccccc'\n",
      "[23:58:31] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:37] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccc(F)c1'\n",
      "[23:58:43] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(c(c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:43] SMILES Parse Error: extra close parentheses while parsing: COc1ncccccccccc(C(=O)Nccc(C(=O)ccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)ccccc4)c4)c4)\n",
      "[23:58:43] SMILES Parse Error: Failed parsing SMILES 'COc1ncccccccccc(C(=O)Nccc(C(=O)ccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)ccccc4)c4)c4)' for input: 'COc1ncccccccccc(C(=O)Nccc(C(=O)ccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)ccccccccccccc(Cl)ccccc4)c4)c4)'\n",
      "[23:58:48] SMILES Parse Error: extra close parentheses while parsing: CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)c2)[C@@H]1NC(=O)ccccc1\n",
      "[23:58:48] SMILES Parse Error: Failed parsing SMILES 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)c2)[C@@H]1NC(=O)ccccc1' for input: 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)c2)[C@@H]1NC(=O)ccccc1'\n",
      "[23:58:54] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(c(c(c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:58:54] SMILES Parse Error: unclosed ring for input: 'COc1ncccccccccc(C(=O)Ncc(C(=O)ccccccc(Cl)cccccccccccccc(Cl)cccccccccccccc(Cl)cccccccccccccc(Cl)ccccc4)c4)c'\n",
      "[23:59:00] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccc(F)c1'\n",
      "[23:59:05] SMILES Parse Error: unclosed ring for input: 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)cccccccccccccccccccc(F)c1'\n",
      "[23:59:11] SMILES Parse Error: extra open parentheses for input: 'COc1ncccccccc2c(C(=O)Nc(Cl)cccccccccccccccccc(Cl)ccccccccccccccccccc(Cl)cccccccccccccccccccc(Cl)ccccccccc'\n",
      "[23:59:11] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'\n",
      "[23:59:17] SMILES Parse Error: unclosed ring for input: 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)cccccccccccccccc(F)c1'\n",
      "[23:59:22] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(N3CCN(C4COC4)CC3)cccccccccccccccccccccc(F)c1'\n",
      "[23:59:28] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(c(N3CCN(C4COC4)CC3)cccccccccccccccccccccccccccccccccccccccccccccccc(F)cccccccccccccc'\n",
      "[23:59:34] SMILES Parse Error: extra close parentheses while parsing: CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)ccc2)[C@@H]1NC(=O)cccccccccccc(F)c1\n",
      "[23:59:34] SMILES Parse Error: Failed parsing SMILES 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)ccc2)[C@@H]1NC(=O)cccccccccccc(F)c1' for input: 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)ccc2)[C@@H]1NC(=O)cccccccccccc(F)c1'\n",
      "[23:59:40] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccc(F)c1'\n",
      "[23:59:45] SMILES Parse Error: extra close parentheses while parsing: COc1nccc2cc(C(=O)Nc(Cl)cccccccccc(C(=O)NCccccccc(Cl)NCccccccccccc4)c4)c4)c4)c4)c4)c(=O)[nH]c2n1\n",
      "[23:59:45] SMILES Parse Error: Failed parsing SMILES 'COc1nccc2cc(C(=O)Nc(Cl)cccccccccc(C(=O)NCccccccc(Cl)NCccccccccccc4)c4)c4)c4)c4)c4)c(=O)[nH]c2n1' for input: 'COc1nccc2cc(C(=O)Nc(Cl)cccccccccc(C(=O)NCccccccc(Cl)NCccccccccccc4)c4)c4)c4)c4)c4)c(=O)[nH]c2n1'\n",
      "[23:59:45] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccccccccc(F)ccccccccccccccccccccccccccccccc'\n",
      "[23:59:51] SMILES Parse Error: extra close parentheses while parsing: CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)c2)[C@@H]1NC(=O)ccccccccccccc(F)c1\n",
      "[23:59:51] SMILES Parse Error: Failed parsing SMILES 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)c2)[C@@H]1NC(=O)ccccccccccccc(F)c1' for input: 'CNC(=O)O[C@H]1COc2c(N3CCN(C4COC4)CC3)c2)[C@@H]1NC(=O)ccccccccccccc(F)c1'\n",
      "[23:59:57] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(N3CCN(C4COC4)CC3)ccccccccccccccccccccccccccc(F)c1'\n",
      "[00:00:02] SMILES Parse Error: extra open parentheses for input: 'CNC(=O)O[C@H]1COc2c(c(c(N3CCN(C4COC4)CC3)cccccccccccccccccccccccccc(F)c1'\n",
      "[00:00:02] SMILES Parse Error: extra close parentheses while parsing: COc1nccc2cc(C(=O)Nc(Cl)cccccccc(C(=O)NCccccc(C(=O)NCc4)c4)c4)c4)c4)c4)c4)c(=O)[nH]c2n1\n",
      "[00:00:02] SMILES Parse Error: Failed parsing SMILES 'COc1nccc2cc(C(=O)Nc(Cl)cccccccc(C(=O)NCccccc(C(=O)NCc4)c4)c4)c4)c4)c4)c4)c(=O)[nH]c2n1' for input: 'COc1nccc2cc(C(=O)Nc(Cl)cccccccc(C(=O)NCccccc(C(=O)NCc4)c4)c4)c4)c4)c4)c4)c(=O)[nH]c2n1'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cfd8a601774a06877891127f46455e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:00:04] SMILES Parse Error: unclosed ring for input: 'CNC(=O)O[C@H]1COc2c(c(N3CCN(C4COC4)CC3)cc2)[C@@H]1NC(=O)cccccccccc(F)c1'\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = MassSpecDataset(\n",
    "    pth=mgf_pth,\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform=None\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = SmilesTransformer(\n",
    "    input_dim=2,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dropout=0.0,\n",
    "    smiles_tokenizer=SmilesBPETokenizer(max_len=200),\n",
    "    k_predictions=1,\n",
    "    pre_norm=False,\n",
    "    max_smiles_len=100,\n",
    "    validate_only_loss=True\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "project = \"MassSpecGymDeNovo\"\n",
    "name = \"SmilesTransformer_debug_overfitting\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=100, logger=logger, log_every_n_steps=1, check_val_every_n_epoch=50\n",
    ")\n",
    "\n",
    "# Validate before training\n",
    "data_module.prepare_data()  # Explicit call needed for validate before fit\n",
    "data_module.setup()  # Explicit call needed for validate before fit\n",
    "trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C/C1=C/CC[C@@]2(C)O[C@@H]2[C@H]2OC(=O)[C@H](CN(C)C)[C@@H]2CC1', 'COc1ncc2cc(C(=O)Nc3c(Cl)ccc(C(=O)NCc4cc(Cl)ccc4)c3)c(=O)[nH]c2n1', 'CNC(=O)O[C@H]1COc2c(cc(N3CCN(C4COC4)CC3)cc2)[C@@H]1NC(=O)c1ccc(F)cc1']\n",
      "[['C/C1=C/CC[C@@]2(C)O[C@@H]2[C@H]2OC(=O)[C@H]2CC1'], ['C/C1=C/CC[C@@]2(C)O[C@@H]2[C@H]2OC(=O)[C@H]2CC1'], ['CNC(=O)O[C@H]1COc2c(c(N3CCN(C4COC4)CC3)c2)[C@@H]1NC(=O)ccccccccccc(F)c1']]\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    batch = next(iter(data_module.train_dataloader()))\n",
    "    print(batch['mol'])\n",
    "    print(model.decode_smiles(batch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De novo random chemical generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "# Load dataset\n",
    "# Uncomment the paths to use debugging data containing only 5 spectra\n",
    "dataset = MassSpecDataset(\n",
    "    pth=mgf_pth,\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform=None\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "# Init model\n",
    "name = \"random_baseline_no_formula\"\n",
    "model = RandomDeNovo(\n",
    "    formula_known=False,\n",
    "    max_top_k=10,\n",
    "    estimate_chem_element_stats=True,\n",
    "    enforce_connectivity=False,\n",
    "    df_test_path=Path(f'../data/test_results/de_novo/{name}.pkl')\n",
    ")\n",
    "\n",
    "# Init logger\n",
    "# You may need to run wandb init first to use the wandb logger\n",
    "# Alternatively set logger = None in Trainer below not to use wandb\n",
    "project = \"MassSpecGymDeNovo\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=1, logger=logger, log_every_n_steps=1000,\n",
    "    limit_val_batches=0, num_sanity_val_steps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory ./MassSpecGymDeNovo/5qzqvgid/checkpoints exists and is not empty.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb764207488e400087e9c13c80f1a33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b99a35d4d15499bae376ecd2b2b5ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_loss                      0.0\n",
      "    test_num_valid_mols                 10.0\n",
      "    test_top_10_accuracy                0.0\n",
      "test_top_10_max_tanimoto_sim    0.07246376574039459\n",
      "   test_top_10_mces_dist                15.0\n",
      "    test_top_1_accuracy                 0.0\n",
      "test_top_1_max_tanimoto_sim     0.07246376574039459\n",
      "    test_top_1_mces_dist                16.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.0,\n",
       "  'test_num_valid_mols': 10.0,\n",
       "  'test_top_1_mces_dist': 16.0,\n",
       "  'test_top_1_max_tanimoto_sim': 0.07246376574039459,\n",
       "  'test_top_1_accuracy': 0.0,\n",
       "  'test_top_10_mces_dist': 15.0,\n",
       "  'test_top_10_max_tanimoto_sim': 0.07246376574039459,\n",
       "  'test_top_10_accuracy': 0.0}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory ./MassSpecGymDeNovo/5qzqvgid/checkpoints exists and is not empty.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "   | Name                          | Type       | Params\n",
      "--------------------------------------------------------------\n",
      "0  | train_num_valid_mols          | MeanMetric | 0     \n",
      "1  | train_top_1_mces_dist         | MeanMetric | 0     \n",
      "2  | train_top_1_max_tanimoto_sim  | MeanMetric | 0     \n",
      "3  | train_top_1_accuracy          | MeanMetric | 0     \n",
      "4  | train_top_10_mces_dist        | MeanMetric | 0     \n",
      "5  | train_top_10_max_tanimoto_sim | MeanMetric | 0     \n",
      "6  | train_top_10_accuracy         | MeanMetric | 0     \n",
      "7  | test_num_valid_mols           | MeanMetric | 0     \n",
      "8  | test_top_1_mces_dist          | MeanMetric | 0     \n",
      "9  | test_top_1_max_tanimoto_sim   | MeanMetric | 0     \n",
      "10 | test_top_1_accuracy           | MeanMetric | 0     \n",
      "11 | test_top_10_mces_dist         | MeanMetric | 0     \n",
      "12 | test_top_10_max_tanimoto_sim  | MeanMetric | 0     \n",
      "13 | test_top_10_accuracy          | MeanMetric | 0     \n",
      "--------------------------------------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# Validate before training\n",
    "data_module.prepare_data()  # Explicit call needed for validate before fit\n",
    "data_module.setup()  # Explicit call needed for validate before fit\n",
    "trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2721c2e4c4412d94c430b4f2a5e048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_loss                      0.0\n",
      "    test_num_valid_mols                 10.0\n",
      "    test_top_10_accuracy                0.0\n",
      "test_top_10_max_tanimoto_sim    0.07246376574039459\n",
      "   test_top_10_mces_dist                15.0\n",
      "    test_top_1_accuracy                 0.0\n",
      "test_top_1_max_tanimoto_sim     0.07246376574039459\n",
      "    test_top_1_mces_dist                16.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.0,\n",
       "  'test_num_valid_mols': 10.0,\n",
       "  'test_top_1_mces_dist': 16.0,\n",
       "  'test_top_1_max_tanimoto_sim': 0.07246376574039459,\n",
       "  'test_top_1_accuracy': 0.0,\n",
       "  'test_top_10_mces_dist': 15.0,\n",
       "  'test_top_10_max_tanimoto_sim': 0.07246376574039459,\n",
       "  'test_top_10_accuracy': 0.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Demo\n",
    "\n",
    "Based on [this script](../scripts/run_simulation.py), using a Fingerprint FFN model.\n",
    "\n",
    "The [demo config](../config/simulation/demo.yml) differs from typical configs (i.e. the [fp_mass config](../config/simulation/fp_mass.yml)) in the following ways: \n",
    "- The model trains on only 1% of the data and stops after 1 epoch\n",
    "- The training occurs on the CPU instead of the GPU\n",
    "- No workers are used for the dataloader\n",
    "- W&B logging is disabled\n",
    "- The retrieval task is disabled\n",
    "\n",
    "As a result of the first point, the simulation results are much worse than those presented in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 420\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-pa ...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /fs01/home/adamo/MassSpecGym_copy/checkpoint/simulation exists and is not empty.\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | FPModel | 87.1 M\n",
      "----------------------------------\n",
      "87.1 M    Trainable params\n",
      "1.6 K     Non-trainable params\n",
      "87.1 M    Total params\n",
      "348.412   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2dd557c0964239a82358783b13b000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc46a6edfc57450696b9b4b3140ad0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ac67762fa343ec9be6881011b9094d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Restoring states from the checkpoint path at /fs01/home/adamo/MassSpecGym_copy/checkpoint/simulation/epoch=000-v3.ckpt\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Loaded model weights from the checkpoint at /fs01/home/adamo/MassSpecGym_copy/checkpoint/simulation/epoch=000-v3.ckpt\n",
      "/h/adamo/miniconda3/envs/MSG2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca135e5fa7b44a808d9cd2c57614d7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_cos_sim          0.10990653187036514\n",
      "    test_cos_sim_obj       0.043753210455179214\n",
      "    test_cos_sim_sqrt      0.043753210455179214\n",
      "     test_hit_rate@1       0.020202020183205605\n",
      "    test_hit_rate@20        0.14141413569450378\n",
      "     test_hit_rate@5        0.05050504952669144\n",
      "       test_js_sim          0.02517317235469818\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_cos_sim': 0.10990653187036514,\n",
       "  'test_js_sim': 0.02517317235469818,\n",
       "  'test_cos_sim_sqrt': 0.043753210455179214,\n",
       "  'test_cos_sim_obj': 0.043753210455179214,\n",
       "  'test_hit_rate@1': 0.020202020183205605,\n",
       "  'test_hit_rate@5': 0.05050504952669144,\n",
       "  'test_hit_rate@20': 0.14141413569450378}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "from massspecgym.data.datasets import SimulationDataset, RetrievalSimulationDataset\n",
    "from massspecgym.data.transforms import SpecToMzsInts, MolToPyG, StandardMeta, MolToFingerprints\n",
    "from massspecgym.models.simulation.fp import FPSimulationMassSpecGymModel\n",
    "from massspecgym.models.simulation.gnn import GNNSimulationMassSpecGymModel\n",
    "from massspecgym.models.simulation.prec_only import PrecOnlySimulationMassSpecGymModel\n",
    "from massspecgym.simulation_utils.run_utils import load_config, get_split_ss\n",
    "\n",
    "# default settings for simulation runs\n",
    "template_fp = \"../config/simulation/template.yml\"\n",
    "# special settings for this demo\n",
    "custom_fp = \"../config/simulation/demo.yml\"\n",
    "\n",
    "# W&B logging is disabled\n",
    "wandb_mode = \"disabled\"\n",
    "# directory for saving model checkpoints\n",
    "checkpoint_dp = \"../checkpoint/simulation\"\n",
    "\n",
    "config_d = load_config(template_fp,custom_fp)\n",
    "\n",
    "pl.seed_everything(config_d[\"seed\"], workers=True)\n",
    "\n",
    "# set torch multiprocessing strategy\n",
    "torch.multiprocessing.set_sharing_strategy(config_d[\"mp_sharing_strategy\"])\n",
    "\n",
    "spec_transform = SpecToMzsInts(\n",
    "    mz_from=config_d[\"mz_from\"],\n",
    "    mz_to=config_d[\"mz_to\"],\n",
    ")\n",
    "if config_d[\"model_type\"] in [\"fp\", \"prec_only\"]:\n",
    "    mol_transform = MolToFingerprints(\n",
    "        fp_types=config_d[\"fp_types\"]\n",
    "    )\n",
    "elif config_d[\"model_type\"] == \"gnn\":\n",
    "    mol_transform = MolToPyG()\n",
    "else:\n",
    "    raise ValueError(f\"model_type {config_d['model_type']} not supported\")\n",
    "meta_transform = StandardMeta(\n",
    "    adducts=config_d[\"adducts\"],\n",
    "    instrument_types=config_d[\"instrument_types\"],\n",
    "    max_collision_energy=config_d[\"max_collision_energy\"]\n",
    ")\n",
    "\n",
    "# wandb\n",
    "wandb.init(\n",
    "    project=config_d[\"wandb_project\"],\n",
    "    entity=config_d[\"wandb_entity\"],\n",
    "    name=config_d[\"wandb_name\"],\n",
    "    mode=wandb_mode,\n",
    "    dir=checkpoint_dp,\n",
    ")\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    entity=config_d[\"wandb_entity\"],\n",
    "    project=config_d[\"wandb_project\"],\n",
    "    name=config_d[\"wandb_name\"],\n",
    "    mode=wandb_mode,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# set up df_test_path\n",
    "save_df_test = config_d.pop(\"save_df_test\")\n",
    "if save_df_test:\n",
    "    df_test_path = os.path.join(wandb.run.dir, \"df_test.pkl\")\n",
    "else:\n",
    "    df_test_path = None\n",
    "config_d[\"df_test_path\"] = df_test_path\n",
    "\n",
    "if config_d[\"model_type\"] == \"fp\":\n",
    "    pl_model = FPSimulationMassSpecGymModel(**config_d)\n",
    "elif config_d[\"model_type\"] == \"prec_only\":\n",
    "    pl_model = PrecOnlySimulationMassSpecGymModel(**config_d)\n",
    "elif config_d[\"model_type\"] == \"gnn\":\n",
    "    pl_model = GNNSimulationMassSpecGymModel(**config_d)\n",
    "else:\n",
    "    raise ValueError(f\"model_type {config_d['model_type']} not supported\")\n",
    "# print(pl_model)\n",
    "\n",
    "ds = SimulationDataset(\n",
    "    pth=config_d[\"pth\"],\n",
    "    meta_keys=config_d[\"meta_keys\"],\n",
    "    spec_transform=spec_transform,\n",
    "    mol_transform=mol_transform,\n",
    "    meta_transform=meta_transform\n",
    ")\n",
    "\n",
    "train_ds, val_ds, test_ds = get_split_ss(\n",
    "    ds,\n",
    "    config_d[\"split_type\"],\n",
    "    subsample_frac=config_d[\"subsample_frac\"]\n",
    ")\n",
    "\n",
    "dl_config = {\n",
    "    \"num_workers\": config_d[\"num_workers\"],\n",
    "    \"batch_size\": config_d[\"batch_size\"],\n",
    "    \"drop_last\": config_d[\"drop_last\"],\n",
    "    \"pin_memory\": config_d[\"pin_memory\"] and config_d[\"accelerator\"] != \"cpu\",\n",
    "    \"persistent_workers\": config_d[\"persistent_workers\"] and config_d[\"accelerator\"] != \"cpu\",\n",
    "    \"collate_fn\": ds.collate_fn\n",
    "}\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, **dl_config)\n",
    "val_dl = DataLoader(val_ds, shuffle=False, **dl_config)\n",
    "test_dl = DataLoader(test_ds, shuffle=False, **dl_config)\n",
    "\n",
    "# NOTE: in this demo, we have disabled the retrieval task\n",
    "if config_d[\"do_retrieval\"]:\n",
    "    # we don't need to create separate datasets, can just overwrite...\n",
    "    ret_ds = RetrievalSimulationDataset(\n",
    "        pth=config_d[\"pth\"],\n",
    "        meta_keys=config_d[\"meta_keys\"],\n",
    "        spec_transform=spec_transform,\n",
    "        mol_transform=mol_transform,\n",
    "        meta_transform=meta_transform,\n",
    "        candidates_pth=config_d[\"candidates_pth\"]\n",
    "    )\n",
    "    ret_dl_config = dl_config.copy()\n",
    "    ret_dl_config[\"batch_size\"] = config_d[\"retrieval_batch_size\"]\n",
    "    ret_dl_config[\"collate_fn\"] = ret_ds.collate_fn\n",
    "    _, _, test_ret_ds = get_split_ss(\n",
    "        ret_ds,\n",
    "        config_d[\"split_type\"],\n",
    "        subsample_frac=config_d[\"subsample_frac\"]\n",
    "    )\n",
    "    test_dl = DataLoader(test_ret_ds, shuffle=False, **ret_dl_config)\n",
    "\n",
    "# checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dp,\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_cos_sim\",\n",
    "    mode=\"max\",\n",
    "    filename=f\"{{epoch:03d}}\",\n",
    "    save_last=False\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=config_d[\"accelerator\"], \n",
    "    max_epochs=config_d[\"max_epochs\"], \n",
    "    logger=logger, \n",
    "    log_every_n_steps=config_d[\"log_every_n_steps\"],\n",
    "    gradient_clip_val=config_d[\"gradient_clip_val\"],\n",
    "    gradient_clip_algorithm=config_d[\"gradient_clip_algorithm\"],\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(\n",
    "    pl_model, \n",
    "    train_dataloaders=train_dl, \n",
    "    val_dataloaders=val_dl\n",
    ")\n",
    "\n",
    "if config_d[\"save_ckpt\"]:\n",
    "    wandb.save(\n",
    "        trainer.checkpoint_callback.best_model_path,\n",
    "        base_path=checkpoint_dp\n",
    "    )\n",
    "\n",
    "# Test\n",
    "trainer.test(\n",
    "    pl_model,\n",
    "    dataloaders=test_dl,\n",
    "    ckpt_path=\"best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSG2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
